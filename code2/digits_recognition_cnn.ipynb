{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digits recognition (using Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ü§ñ See [full list of Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) on **GitHub**<br/><br/>\n",
    "> - ‚ñ∂Ô∏è **Interactive Demo**: [try this model and other machine learning experiments in action](https://trekhleb.github.io/machine-learning-experiments/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will build a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) model using [Tensorflow](https://www.tensorflow.org/) to recognize handwritten digits.\n",
    "\n",
    "A **convolutional neural network** (CNN, or ConvNet) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n",
    "\n",
    "![digits_recognition_cnn.png](../../demos/src/images/digits_recognition_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "\n",
    "- [tensorflow](https://www.tensorflow.org/) - for developing and training ML models.\n",
    "- [matplotlib](https://matplotlib.org/) - for plotting the data.\n",
    "- [seaborn](https://seaborn.pydata.org/index.html) - for plotting confusion matrix.\n",
    "- [numpy](https://numpy.org/) - for linear algebra operations.\n",
    "- [pandas](https://pandas.pydata.org/) - for displaying training/test data in a table.\n",
    "- [math](https://docs.python.org/3/library/math.html) - for calculating square roots etc.\n",
    "- [datetime](https://docs.python.org/3.8/library/datetime.html) - for generating a logs folder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Tensorflow version v2 (the command is relevant for Colab only).\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 01:04:47.031202: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 01:04:47.032532: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-21 01:04:47.059405: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-21 01:04:47.059844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 01:04:47.586408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.9\n",
      "Tensorflow version: 2.12.0\n",
      "Keras version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import platform\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Tensorboard\n",
    "\n",
    "We will use [Tensorboard](https://www.tensorflow.org/tensorboard) to debug the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "# %reload_ext tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs.\n",
    "!rm -rf ./.logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The **training** dataset consists of 60000 28x28px images of hand-written digits from `0` to `9`.\n",
    "\n",
    "The **test** dataset consists of 10000 28x28px images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "x_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_WIDTH: 28\n",
      "IMAGE_HEIGHT: 28\n",
      "IMAGE_CHANNELS: 1\n"
     ]
    }
   ],
   "source": [
    "# Save image parameters to the constants that we will use later for data re-shaping and for model traning.\n",
    "(_, IMAGE_WIDTH, IMAGE_HEIGHT) = x_train.shape\n",
    "IMAGE_CHANNELS = 1\n",
    "\n",
    "print('IMAGE_WIDTH:', IMAGE_WIDTH);\n",
    "print('IMAGE_HEIGHT:', IMAGE_HEIGHT);\n",
    "print('IMAGE_CHANNELS:', IMAGE_CHANNELS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "\n",
    "Here is how each image in the dataset looks like. It is a 28x28 matrix of integers (from `0` to `255`). Each integer represents a color of a pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>26</td>\n",
       "      <td>166</td>\n",
       "      <td>255</td>\n",
       "      <td>247</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>172</td>\n",
       "      <td>253</td>\n",
       "      <td>242</td>\n",
       "      <td>195</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>238</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>56</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>219</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>249</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>207</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>250</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>171</td>\n",
       "      <td>219</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>172</td>\n",
       "      <td>226</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3    4    5    6    7    8    9   ...   18   19   20   21  \\\n",
       "0    0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "5    0   0   0   0    0    0    0    0    0    0  ...  175   26  166  255   \n",
       "6    0   0   0   0    0    0    0    0   30   36  ...  225  172  253  242   \n",
       "7    0   0   0   0    0    0    0   49  238  253  ...   93   82   82   56   \n",
       "8    0   0   0   0    0    0    0   18  219  253  ...    0    0    0    0   \n",
       "9    0   0   0   0    0    0    0    0   80  156  ...    0    0    0    0   \n",
       "10   0   0   0   0    0    0    0    0    0   14  ...    0    0    0    0   \n",
       "11   0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "12   0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "13   0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "14   0   0   0   0    0    0    0    0    0    0  ...   25    0    0    0   \n",
       "15   0   0   0   0    0    0    0    0    0    0  ...  150   27    0    0   \n",
       "16   0   0   0   0    0    0    0    0    0    0  ...  253  187    0    0   \n",
       "17   0   0   0   0    0    0    0    0    0    0  ...  253  249   64    0   \n",
       "18   0   0   0   0    0    0    0    0    0    0  ...  253  207    2    0   \n",
       "19   0   0   0   0    0    0    0    0    0    0  ...  250  182    0    0   \n",
       "20   0   0   0   0    0    0    0    0    0    0  ...   78    0    0    0   \n",
       "21   0   0   0   0    0    0    0    0   23   66  ...    0    0    0    0   \n",
       "22   0   0   0   0    0    0   18  171  219  253  ...    0    0    0    0   \n",
       "23   0   0   0   0   55  172  226  253  253  253  ...    0    0    0    0   \n",
       "24   0   0   0   0  136  253  253  253  212  135  ...    0    0    0    0   \n",
       "25   0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "26   0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "27   0   0   0   0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "     22   23  24  25  26  27  \n",
       "0     0    0   0   0   0   0  \n",
       "1     0    0   0   0   0   0  \n",
       "2     0    0   0   0   0   0  \n",
       "3     0    0   0   0   0   0  \n",
       "4     0    0   0   0   0   0  \n",
       "5   247  127   0   0   0   0  \n",
       "6   195   64   0   0   0   0  \n",
       "7    39    0   0   0   0   0  \n",
       "8     0    0   0   0   0   0  \n",
       "9     0    0   0   0   0   0  \n",
       "10    0    0   0   0   0   0  \n",
       "11    0    0   0   0   0   0  \n",
       "12    0    0   0   0   0   0  \n",
       "13    0    0   0   0   0   0  \n",
       "14    0    0   0   0   0   0  \n",
       "15    0    0   0   0   0   0  \n",
       "16    0    0   0   0   0   0  \n",
       "17    0    0   0   0   0   0  \n",
       "18    0    0   0   0   0   0  \n",
       "19    0    0   0   0   0   0  \n",
       "20    0    0   0   0   0   0  \n",
       "21    0    0   0   0   0   0  \n",
       "22    0    0   0   0   0   0  \n",
       "23    0    0   0   0   0   0  \n",
       "24    0    0   0   0   0   0  \n",
       "25    0    0   0   0   0   0  \n",
       "26    0    0   0   0   0   0  \n",
       "27    0    0   0   0   0   0  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix of numbers may be drawn as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some more training examples to get the feeling of how the digits were written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numbers_to_display = 25\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(numbers_to_display):\n",
    "    plt.subplot(num_cells, num_cells, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the data\n",
    "\n",
    "In order to use convolution layers we need to reshape our data and add a color channel to it. As you've noticed currently every digit has a shape of `(28, 28)` which means that it is a 28x28 matrix of color values form `0` to `255`. We need to reshape it to `(28, 28, 1)` shape so that each pixel potentially may have multiple channels (like Red, Green and Blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_with_chanels = x_train.reshape(\n",
    "    x_train.shape[0],\n",
    "    IMAGE_WIDTH,\n",
    "    IMAGE_HEIGHT,\n",
    "    IMAGE_CHANNELS\n",
    ")\n",
    "\n",
    "x_test_with_chanels = x_test.reshape(\n",
    "    x_test.shape[0],\n",
    "    IMAGE_WIDTH,\n",
    "    IMAGE_HEIGHT,\n",
    "    IMAGE_CHANNELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_with_chanels: (60000, 28, 28, 1)\n",
      "x_test_with_chanels: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train_with_chanels:', x_train_with_chanels.shape)\n",
    "print('x_test_with_chanels:', x_test_with_chanels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "\n",
    "Here we're just trying to move from values range of `[0...255]` to `[0...1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = x_train_with_chanels / 255\n",
    "x_test_normalized = x_test_with_chanels / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.18039216],\n",
       "       [0.50980392],\n",
       "       [0.71764706],\n",
       "       [0.99215686],\n",
       "       [0.99215686],\n",
       "       [0.81176471],\n",
       "       [0.00784314],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check just one row from the 0th image to see color chanel values after normalization.\n",
    "x_train_normalized[0][18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "We will use [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=stable) Keras model.\n",
    "\n",
    "Then we will have two pairs of [Convolution2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?version=stable) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D?version=stable) layers. The MaxPooling layer acts as a sort of downsampling using max values in a region instead of averaging.\n",
    "\n",
    "After that we will use [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten?version=stable) layer to convert multidimensional parameters to vector.\n",
    "\n",
    "The las layer will be a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable) layer with `10` [Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax?version=stable) outputs. The output represents the network guess. The 0-th output represents a probability that the input digit is `0`, the 1-st output represents a probability that the input digit is `1` and so on...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 01:04:50.290032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),\n",
    "    kernel_size=5,\n",
    "    filters=8,\n",
    "    strides=1,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "    name= 'conv1'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2),\n",
    "    name= 'pool_1'\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    kernel_size=5,\n",
    "    filters=16,\n",
    "    strides=1,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "    name= 'conv2'\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2),\n",
    "    name= 'pool_2'\n",
    "))\n",
    "#model.add(tf.keras.layers.Flatten(input_shape=x_train_normalized.shape[1:], name='flatten'))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    name = 'dense1'\n",
    "));\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2, name ='dropout1' ))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation=tf.keras.activations.softmax,\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling() ,\n",
    "    name = 'output'\n",
    "))\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "# filters_per_conv_layer = [16, 16, 24]\n",
    "# x = x_in = Input(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "\n",
    "# for i, f in enumerate(filters_per_conv_layer):\n",
    "#     print(('Adding convolutional block {} with N={} filters').format(i, f))\n",
    "#     x = Conv2D(\n",
    "#         int(f),\n",
    "#         kernel_size=(5),\n",
    "#         strides=(1, 1),\n",
    "#         kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "#         name='conv_{}'.format(i),\n",
    "#     )(x)\n",
    "#     x = Activation('relu', name='conv_act_%i' % i)(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2),strides =(2,2) , name='pool_{}'.format(i))(x)\n",
    "#     x = Conv2D(\n",
    "#         int(f),\n",
    "#         kernel_size=(3),\n",
    "#         strides=(1, 1),\n",
    "#         kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "#         name='conv_{}'.format(i),\n",
    "#     )(x)\n",
    "        \n",
    "        \n",
    "#     x = BatchNormalization(name='bn_conv_{}'.format(i))(x)\n",
    "#     x = Activation('relu', name='conv_act_%i' % i)(x)\n",
    "#     x = MaxPooling2D(pool_size=(2, 2), name='pool_{}'.format(i))(x)\n",
    "# x = Flatten()(x)\n",
    "\n",
    "# for i, n in enumerate(neurons_per_dense_layer):\n",
    "#     print(('Adding dense block {} with N={} neurons').format(i, n))\n",
    "#     x = Dense(n, kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001), name='dense_%i' % i, use_bias=False)(x)\n",
    "#     x = BatchNormalization(name='bn_dense_{}'.format(i))(x)\n",
    "#     x = Activation('relu', name='dense_act_%i' % i)(x)\n",
    "# x = Dense(int(n_classes), name='output_dense')(x)\n",
    "# x_out = Activation('softmax', name='output_softmax')(x)\n",
    "\n",
    "# model = Model(inputs=[x_in], outputs=[x_out], name='keras_baseline')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our model summary so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (None, 24, 24, 8)         208       \n",
      "                                                                 \n",
      " pool_1 (MaxPooling2D)       (None, 12, 12, 8)         0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 8, 8, 16)          3216      \n",
      "                                                                 \n",
      " pool_2 (MaxPooling2D)       (None, 4, 4, 16)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 10)                2570      \n",
      "                                                                 \n",
      " dropout1 (Dropout)          (None, 10)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,104\n",
      "Trainable params: 6,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the model the `graphviz` should be installed. For Mac OS it may be installed using `brew` like `brew install graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7475 - accuracy: 0.7333 - val_loss: 0.1453 - val_accuracy: 0.9608\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3657 - accuracy: 0.8731 - val_loss: 0.1016 - val_accuracy: 0.9736\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3086 - accuracy: 0.8945 - val_loss: 0.0829 - val_accuracy: 0.9783\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2696 - accuracy: 0.9123 - val_loss: 0.0700 - val_accuracy: 0.9818\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2225 - accuracy: 0.9289 - val_loss: 0.0646 - val_accuracy: 0.9824\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2038 - accuracy: 0.9351 - val_loss: 0.0639 - val_accuracy: 0.9827\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1922 - accuracy: 0.9372 - val_loss: 0.0651 - val_accuracy: 0.9826\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1850 - accuracy: 0.9396 - val_loss: 0.0635 - val_accuracy: 0.9835\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1809 - accuracy: 0.9421 - val_loss: 0.0636 - val_accuracy: 0.9843\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1739 - accuracy: 0.9432 - val_loss: 0.0587 - val_accuracy: 0.9839\n"
     ]
    }
   ],
   "source": [
    "log_dir=\".logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "training_history = model.fit(\n",
    "    x_train_normalized,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test_normalized, y_test),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the loss function was changing during the training. We expect it to get smaller and smaller on every next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f442a8aa770>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(training_history.history['loss'], label='training set')\n",
    "plt.plot(training_history.history['val_loss'], label='test set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f43c823c130>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQFklEQVR4nO3deVyU5d4G8GtmYJhhG3bERMAtUdRUFAW3NlDT0lNpliRleuxYymv1djxlHc0Tqe0aHLVcOq+pp8XynDSXNAU3FMWNUlQUFxBB2bdh5nn/GGZ0HEBGB55Zru/nMx9m7meZ30A1V/dzP/ctEQRBABEREZEDkYpdABEREVFrYwAiIiIih8MARERERA6HAYiIiIgcDgMQERERORwGICIiInI4DEBERETkcJzELsAaabVaXLlyBR4eHpBIJGKXQ0RERM0gCALKysrQtm1bSKVN9/EwADXgypUrCA4OFrsMIiIiugsXL15Eu3btmtyHAagBHh4eAHS/QE9PT5GrISIiouYoLS1FcHCw4Xu8KQxADdBf9vL09GQAIiIisjHNGb7CQdBERETkcBiAiIiIyOEwABEREZHDYQAiIiIih8MARERERA6HAYiIiIgcDgMQERERORwGICIiInI4DEBERETkcBiAiIiIyOEwABEREZHDYQAiIiIih8PFUImIiKyNIABaDaBVA5paQFOn+6mtAyQSQCIFJDJAKqt/Lq1/LjN+LmU/R2MYgIiIyH4JAqDRh4ha3XOTUKGu30d9M2To9zXaX93ENnPP1Yz9IVjmd2AUlvQB6fYApX8uaWDf234abZfcErZu3S698/t6hwIxMy3zGe8CAxAREZlP30NhCBa3Pm4LHHU1pm2aWkDTWHstUNfIuZo8Tv9et7Rp1WL/pixHIgWkzgDqf/eCpnnHCVrdw9p+F+36MwARkYi0Wt3/dRo9NI28VjeyvZFj9P/H3OQ5GziP/j/YhodQ/7Ohbbdub+xx23aT9zDzeMND08T2+v97l0jqf9GSu3iO29olt5zTEs9h2n57LYJws5fi9iBjqR6KViUBZM6ATA5InXQ/Zc66h7S+XVbfLnW+bZvz3R1reN3YtsbOddt7S2WmH0e4JQwZ/rnWGP8zbrL9ljaT57cff5fn0mrq/71o4lyebVv/z38LBiAiWyAIQHUxUF4AlF+95ectz6tLbwkWZgQPQSv2pyN7IZMDMpebX+yGL/H6n04ut7Xd/ryhtrs8zum2dn2o0F/msRcSiS5Y8evcbPyNEYmptrLxQKP/WXFN91NT27q1SZ1uecgaee18h+23vJY5N7H9ljajsQT6wZ63P+60/dZHE/sYxjzcwzmaOkbfCwTBzOdooF24pd0Sz9H0PrfW0pwgI3Wyr2BBdo8BiMjSNOr60FLQdLApLwBqy8w7t0IFuAfWPwKMfyq8bnaTmxVWGgks/DIjIjvGAETUHIIAVN1oIMw0EGwqi8w7t5OigVBz+3N/wC0AcFa0zOcjInIwDEDk2NRVQOkV08tNJsGmwLw7KCQywM2/kUBzW5uLB3tbiIhaGQMQ2Td1FVByCSi+ANy4ABTnGj8qCsw7n9LbNMS4+Zu2ufo0fMcGERFZBQYgsm3q6psBxxBsbnlefvXO53BSAh5NXYIKuBl0nFxa/jMREVGLYwAi61ZX00DAyb3Zm1Oef+dzOLsB3iGAV/tbHre8VnrzEhQRkYNhACJx1dUCpZcavjxVnAuU5eGOk605u90WbtrfEnhCGHCIiMgEAxC1LI26vgengctTxbm6Ach3DDiupgHH65aA4+rDgENERGZhAKJ7o1EDpZdNe270PTplV+4807CTsoGAo+/FCQFcfRlwiIjIohiAyDzVJcAfPwNZPwFXs3Th504L8jkpGg44XqG6n25+DDhERNSqGIDozmorgFObgRM/AGe2mS7JIHNpJOCE6Hpx3PwZcIiIyKowAFHD1NW6sHPie+DUL0Bd1c1tfvcDEU8CHYYB3qG6gCOVilUpERGR2RiA6Ka6WuDcTl1Pzx8/G69T5R2mCz0RfwICurFHh4iIbBoDkKPT1AHnU3U9Pb//B6guvrnNsx0QMVYXfIIeYOghIiK7wQDkiLRaIHcfcPIH3WDmims3t7kHAt3HAt3/BLTrx0tbRERklxiAHIUgAJczdJe3Tm7Q3Z6up/QBuj2hu7wVEsM1rIiIyO4xANkzQQDyj+sub538QTcvj56LCggfpevp6TAUkDmLVycREVErYwCyRwV/6ALPiR+Aouyb7c5uwP0jdGN6Oj3MhT2JiMhhMQDZi+vndIHnxA9Awcmb7TIXoEusLvR0jgPkruLVSEREZCUYgGxZ8UXdeJ6TPwBXjtxslzrreni6/0nX46PwFK9GIiIiK8QAZGvKrgJZP+p6ei7uv9kukQFhQ3Q9PeGjdCugExERUYMYgGxBRRHw+0bdYOYLe25ZXFQChETr7t4KfwJw9xe1TCIiIlvBAGSt9IuOnvgeOLvTeMHRdv10l7e6jwE824pWIhERka1iALImNeXA6V8aXnS0TU/d5a3uY3ULjBIREdFdYwASm7oKyK5fdPT0FtNFR3s8pevt8eskXo1ERER2hgFIDHW1wNkduru3/vgZqC2/uY2LjhIREbU4BqDWVPAHsG8JFx0lIiISGQNQa6osAo78S/eci44SERGJhgGoNbUfCAx8BegSx0VHiYiIRMQA1JqkUiDuH2JXQURE5PB43YWIiIgcDgMQERERORwGICIiInI4DEBERETkcBiAiIiIyOEwABEREZHDYQAiIiIih8MARERERA5H9ACUnJyMsLAwKBQK9O3bF6mpqU3u/8UXXyA8PBxKpRL3338/vv76a6Ptq1atgkQiMXlUV1e35McgIiIiGyLqTNDr169HYmIikpOTERMTg6VLl2LEiBHIyspC+/btTfZPSUnB7NmzsXz5cvTr1w/p6emYMmUKvL29MXr0aMN+np6eOHXqlNGxCoWixT8PERER2QaJIAiCWG8eFRWFPn36ICUlxdAWHh6OMWPGICkpyWT/6OhoxMTEYNGiRYa2xMREHDp0CGlpaQB0PUCJiYkoLi6+67pKS0uhUqlQUlICT0/Puz4PERERtR5zvr9FuwRWW1uLjIwMxMbGGrXHxsZi7969DR5TU1Nj0pOjVCqRnp4OtVptaCsvL0dISAjatWuHUaNG4ciRI03WUlNTg9LSUqMHERER2S/RAlBhYSE0Gg0CAwON2gMDA5Gfn9/gMXFxcfjyyy+RkZEBQRBw6NAhrFixAmq1GoWFhQCArl27YtWqVdi4cSPWrl0LhUKBmJgYZGdnN1pLUlISVCqV4REcHGy5D0pERERWR/RB0BKJxOi1IAgmbXpz5szBiBEjMGDAADg7O+OJJ55AQkICAEAmkwEABgwYgIkTJ6JXr14YPHgw/v3vf6NLly5YvHhxozXMnj0bJSUlhsfFixct8+GIiIjIKokWgPz8/CCTyUx6ewoKCkx6hfSUSiVWrFiByspKnD9/Hrm5uQgNDYWHhwf8/PwaPEYqlaJfv35N9gC5uLjA09PT6EFERET2S7QAJJfL0bdvX2zbts2ofdu2bYiOjm7yWGdnZ7Rr1w4ymQzr1q3DqFGjIJU2/FEEQUBmZiaCgoIsVjsRERHZNlFvg581axbi4+MRGRmJgQMHYtmyZcjNzcW0adMA6C5NXb582TDXz+nTp5Geno6oqCjcuHEDH3/8MU6cOIHVq1cbzjl37lwMGDAAnTt3RmlpKT7//HNkZmbiiy++EOUzEhERkfURNQCNHz8eRUVFmDdvHvLy8hAREYFNmzYhJCQEAJCXl4fc3FzD/hqNBh999BFOnToFZ2dnPPjgg9i7dy9CQ0MN+xQXF2Pq1KnIz8+HSqVC7969sXv3bvTv37+1Px4RERFZKVHnAbJWnAeIiIjI9pjz/S1qDxARERHZP0EQUKvRoqZOi9o63U+ZRII2KvFWaWAAIiIislN1t4SOWo0WNWotajUaVKt1r/VhRPdTo9uvgbYaw7E3f9bWaYwCTePn0O1/u36h3vh2WtM3PbUkBiAiIiKRCYKAyloNiqvUKKlUo7iqFiWVapRUqXVtVWoUV6pRWq1Gda3G0Jtyp/CitdJBLnInKWTShuf8ay0MQERERBai1mgNYaWkqvaW5zd/6p7XGsJNaf22uhZOKzKpBHKZFC7OUuOfTjLInaSQO0nhYnjUt5nsLzO81u8vr9/fxcm0TW7Spju2sQmPWxMDEBER0S0EQUBZTd3NHhhDT4wutJRU3t5Wh5L6QFNRq7mn93aWSaBSyuHl6gyV0hleSt1PlaszvJRyeCqdoHSW3Qwj9cHiTmFELpPCSSb64g9WhQGIiIjsjv6SUmm1GqVVdSY9LyVVt4cbfU+Mbvu9dsZ4Kpzg5SrXhRhXZ3jeEmb04cYo6NT/VDrLrKJ3xBEwABERkVWqVmtQWqUb91JSVVcfZuof1XW3bNOFHMP2+m33eklJ4Syt74WRG3ph9L0yhhCjDzm3hBsPhbPo41vozhiAiIioRdTWaU1CSUl9aLk9sJQYgs3NbbV1pncOmctJKoFK6QwPhRNUrvIGemH0r40vO3kqnaFwllngt0DWigGIiIgaVKfRoqy6ziiUNBRUbgabOqNtVep7Gw8DAFIJ4Kl0hqfCGZ5KJ91PhS6oGF7XP1cZ9ru5Py8pUWMYgIiI7IwgCKhWa1FWrUZZTR3Kqut0z6vrUF4faMqq61Bec7O9rLquft+b+1kiwACAh+KWoKJwgmd9r4tRqDG0OdUHGt1zN7kTpLycRC2AAYiIyIpotIIhmJTfFl5uPhrZVnMzvFjylmo3ucykF0alNA40t/bEGLYrnOGucOJ4GLJKDEBERBZWUqnGxRuVhvEtxj0tuvBSWh9ayo3Czb3fRn0rqQRwd3GCh0I3Bkb30D03bXeCh4susOh7bDwUTnBzcYIzb58mO8QARER0l65X1CL7ahmyC8pxpqAcp+ufXyuruedzy52k8KwPLLqwcjPAuLs43dzWSLuHwgmuco5/IWoMAxARURMEQUBheS2yC8pwpqAc2VfLkV1Qhuyr5SiqqG30uAAPF3i5Ot/W01L/sz7QuN/aA+Ny87m7wgkuTrwDiaglMQAREUEXdArKam4GnIJynKl/fqNS3ehxwT5KdA7wQOcAd3QKcEeXQA90DHCHuwv/80pkzfhvKBE5FEEQkFdSjeyCcmRfre/VqX9eWl3X4DESCRDi44pOAR7oHOiOzgHu6BzggY4BbnCV8z+jRLaI/+YSkV3SagVcLq6qDzhl9T07urE65TUNBx2pBAj1dasPObqw0ynAHR393TkpHpGdYQAiIpum1Qq4eKPSEHD0Y3XOFJSjspE7qpykEoT6uel6cgI96n+6I8zPjWNviBwEAxAR2QSNVkDu9Uqc1l+2qr/j6uy1clSrG14ywVkmQQc/d3QKdEeXWy5fhfi6Qe7EW7uJHBkDEBFZFbVGiwtFlYaAox+fc66wotG1oeROUnT014/Nqe/VCXRHiI8rnDiHDRE1gAGIiEQnCALSzhTiq7Qc7DlTCLWm4VmMFc5SdKofgNypPux0CfRAsI8rZxsmIrMwABGRaKrVGvyUeRkr0s7j1NUyQ7urXFZ/W7nxXVftvJVcF4qILIIBiIha3bWyGvxr/wWs2X/BMJmgq1yGcZHBmDigPTr4uTPoEFGLYgAiolbzR34pvkrNwU+ZV1Cr0Y3naatSICEmFOP7tYdK6SxyhUTkKBiAiKhFabUCdp2+hq/ScpB2ptDQ/kCwFyYPCsPwiDZcbJOIWh0DEBG1iKpaDb4/fAkr9uTg3LUKALqJBodHtMHkQR3QN8Rb5AqJyJExABGRRV0trcbqvefxTXouiuvX0PJwccL4fsGYFB2KYB9XkSskImIAIiILOXG5BF+l5eC/x64YbmMP9lHihegwPB3ZDh4Kju8hIuvBAEREd02jFfDr71fxZVoO0nOuG9r7hXpj8qAwPNqtDefnISKrxABERGarqKnDt4cuYuXe87hQVAlAt77WYz2DMHlQGHq28xK3QCKiO2AAIqJmu1JcZRjfU1atW1HdU+GEZ6NCMCk6BEEqpcgVEhE1DwMQEd3Rkdwb+CotB5tP5EOj1Y3vCfNzw4sxoXiybzu4yvmfEiKyLfyvFhE1qE6jxdasq/gy9RwO5xYb2gd28MXkQWF4qGsAZ2smIpvFAERERkqr1fj3wYtYuec8LhdXAQCcZRI83us+vDgoFN3bqkSukIjo3jEAEREA4OL1Sqzccx7/PnQR5TW68T3ers6YOCAE8QNCEOCpELlCIiLLYQAicmCCIODQhRv4KjUHW7PyUT+8B50C3DF5UBjG9r4PCmeZuEUSEbUABiAiB6TWaLHpeB6+SsvBsUslhvbBnf0weVAYhnT25/geIrJrDEBEDqSkUo1v0nPx9b7zyCupBgDInaQY+8B9eHFQGO5v4yFyhURErYMBiMgBnLtWjpV7zuO7jEuoUmsAAH7ucsQPCMVzA9rDz91F5AqJiFoXAxCRnRIEAfvOFWFFWg5+/aMAQv34nq5tPDB5UBgef6AtXJw4voeIHBMDEJGdqa3T4j9Hr+CrtBxk5ZUa2h/qGoDJg8IQ3dEXEgnH9xCRY2MAIrIT1ytqsWb/BXy9/wKuldUAABTOUjzVtx1eiAlDR393kSskIrIeDEBENi63qBJLd5/FdxmXUFOnBQAEerrg+YGheLZ/e3i7yUWukIjI+jAAEdmoU/llSPntDP5zLM+wPlfEfZ54aVAHjOwRBLmTVOQKiYisFwMQkY05knsDX+w8i+2/XzW0Denij5eHdsSADj4c30NE1AwMQEQ2QBAE7D1bhC92nsHes0UAAIkEGBHRBi8P7YQe7bg+FxGRORiAiKyYVitg2+9XkfzbWRy9WAwAcJJKMKb3fZg2tCM6BXBgMxHR3WAAIrJCdRot/nPsCpJ3nkV2QTkAwMVJign922PKkA64z0spcoVERLaNAYjIilSrNfg24xKW7jqLSzeqAAAeLk6IHxiCFweFccZmIiILYQAisgLlNXVYs/8CvkzLMczh4+smx4uDwhA/MASeCmeRKyQisi8MQEQiul5Ri1V7crBq73mUVtcBANqqFJg6pAPG92sPpZxLVRARtQQGICIR5JdUY3nqOXxzINewOGkHfze8PLQjnnjgPs7hQ0TUwhiAiFrR+cIK/HPXWXx/+BLUmpuTF/5lWCfEdW8DmZRz+BARtQYGIKJW8HteKZJ/O4ufj11B/aTN6B/mg+kPdsKQzn6cvJCIqJUxABG1oIwL1/HFzrPY8UeBoe2hrgH4y7COiAz1EbEyIiLHxgBEZGGCIGB3diGSd57BgZzrAHSzNj/WIwgvD+uI7m05azMRkdhEH2mZnJyMsLAwKBQK9O3bF6mpqU3u/8UXXyA8PBxKpRL3338/vv76a5N9vv/+e3Tr1g0uLi7o1q0bNmzY0FLlExlotQI2H8/D6CVpmLQiHQdyrsNZJsEz/YKx47VhWPJsH4YfIiIrIWoP0Pr165GYmIjk5GTExMRg6dKlGDFiBLKystC+fXuT/VNSUjB79mwsX74c/fr1Q3p6OqZMmQJvb2+MHj0aALBv3z6MHz8e7733HsaOHYsNGzZg3LhxSEtLQ1RUVGt/RHIAao0WP2VeQcpvZ3D2WgUAQOksq5+1OQxBKs7aTERkbSSCIAhivXlUVBT69OmDlJQUQ1t4eDjGjBmDpKQkk/2jo6MRExODRYsWGdoSExNx6NAhpKWlAQDGjx+P0tJSbN682bDP8OHD4e3tjbVr1zarrtLSUqhUKpSUlMDT0/NuPx7ZuWq1BusPXsSy3edwuVg3a7OnwgkJ0aFIiAmDj5tc5AqJiByLOd/fovUA1dbWIiMjA3/961+N2mNjY7F3794Gj6mpqYFCoTBqUyqVSE9Ph1qthrOzM/bt24f/+Z//MdonLi4On376aaO11NTUoKamxvC6tLTUzE9DjqS0Wo3/238BK9JyUFheCwDwc3fBS4PD8FxUe3hw1mYiIqsnWgAqLCyERqNBYGCgUXtgYCDy8/MbPCYuLg5ffvklxowZgz59+iAjIwMrVqyAWq1GYWEhgoKCkJ+fb9Y5ASApKQlz58699w9Fdq2wvAYr9+Tg670XUFajm7W5nbcSfx7aEU/3bQeFM2dtJiKyFaLfBXb7/CeCIDQ6J8qcOXOQn5+PAQMGQBAEBAYGIiEhAQsXLoRMdvPLx5xzAsDs2bMxa9Ysw+vS0lIEBwffzcchO3SluArLdp/DuoO5qFZrAQCdAtzxl2EdMbpXWzjLRL+XgIiIzCRaAPLz84NMJjPpmSkoKDDpwdFTKpVYsWIFli5diqtXryIoKAjLli2Dh4cH/Pz8AABt2rQx65wA4OLiAhcXrrJNxs5eK8c/fzuLDUcuo65+9sKe7VSY/mAnPBoeCClnbSYislmi/a+rXC5H3759sW3bNqP2bdu2ITo6usljnZ2d0a5dO8hkMqxbtw6jRo2CVKr7KAMHDjQ559atW+94TiK9E5dLMH3NYTzy8S58m3EJdVoBAzv44v8mR+Gn6TGI696G4YeIyMaJegls1qxZiI+PR2RkJAYOHIhly5YhNzcX06ZNA6C7NHX58mXDXD+nT59Geno6oqKicOPGDXz88cc4ceIEVq9ebTjnzJkzMWTIECxYsABPPPEEfvrpJ2zfvt1wlxhRY9JzruOLnWew6/Q1Q9sj4YH4y4Md0ae9t4iVERGRpYkagMaPH4+ioiLMmzcPeXl5iIiIwKZNmxASEgIAyMvLQ25urmF/jUaDjz76CKdOnYKzszMefPBB7N27F6GhoYZ9oqOjsW7dOrz99tuYM2cOOnbsiPXr13MOIGqQIAj47fQ1JO88g4PnbwAApBJgdK+2eHlYR3Rtw2kQiIjskajzAFkrzgPkGNJzrmP+z1k4dqkEACCXSfFUZDv8eUgHhPi6iVwdERGZyybmASISy8Xrlfhg8x/4+XgeAMBVLsPEASGYPCgMgZ6KOxxNRET2gAGIHEZ5TR1SfjuD5ak5qK3TQioBnunfHrMe7QI/d94FSETkSBiAyO5ptQK+P3wJC7ecwrUy3YzfAzv44p3R3RAexEucRESOiAGI7NrB89cx7z9ZOH5ZN84nxNcVb40Mx6PdApucHJOIiOwbAxDZpYvXK/HBL3/g52O6cT4eLk549eFOmBQdChcnLllBROToGIDIrlTU1CH5tnE+4/u1x2uxHOdDREQ3MQCRXWhsnM+cUd3QrS3H+RARkTEGILJ5DY3z+dvIcMRynA8RETWCAYhs1qUblUjafHOcj7uLE159qBMSYjjOh4iImsYARDanoqYOKb+dxbLUc6it00IiAZ7pp5vPx9+D43yIiOjOGIDIZmi1An44chkLf/kDBfXjfAZ08ME7o7pznA8REZmFAYhswqHz1zHvvzfX7WrvoxvnE9ed43yIiMh8DEBk1S7d0K3b9V+O8yEiIgtiACKrVFFTh3/uOotlu8+hxjDOJxizHr2f43yIiOieMQCRVdFqBWw4chkLt/yBq6U3x/nMGdUN3duqRK6OiIjsBQMQWQ2O8yEiotbCAESiu1xchQ82/4H/HL0CQDfO55WHOuEFjvMhIqIWwgBEoqmoqcPSXWex9JZxPuMjg/FaLMf5EBFRy2IAolbX0DifqDAfvDOa43yIiKh1MABRq8q4oFu362j9OJ9gHyXeGhmOuO5tOM6HiIhajdkBKDQ0FC+++CISEhLQvn37lqiJ7FBD43ymP6gb56Nw5jgfIiJqXVJzD3jttdfw008/oUOHDnj00Uexbt061NTUtERtZAcqa+vw8dZTeOjD3/Cfo1cM43x2vD4ULw/ryPBDRESikAiCINzNgUePHsWKFSuwdu1a1NXV4dlnn8WLL76IPn36WLrGVldaWgqVSoWSkhJ4enKNqbuh1Qr4MfMyFvxyc5xP/zAfvDOqGyLu4zgfIiKyPHO+v+86AOmp1WokJyfjzTffhFqtRkREBGbOnIkXXnjBZsd0MADdm4wLNzDvv1k4erEYgG6cz99GhGN4BMf5EBFRyzHn+/uuB0Gr1Wps2LABK1euxLZt2zBgwABMnjwZV65cwVtvvYXt27fjm2++udvTkw26Uj/OZ2P9OB83uQyvPNSZ43yIiMjqmB2ADh8+jJUrV2Lt2rWQyWSIj4/HJ598gq5duxr2iY2NxZAhQyxaKFmvyto6/HPXOSzbfRbVat18PuP6BuO1uC4I8FCIXR4REZEJswNQv3798OijjyIlJQVjxoyBs7OzyT7dunXDM888Y5ECyXpptQJ+OnoZCzafQn5pNQCO8yEiIttgdgA6d+4cQkJCmtzHzc0NK1euvOuiyPodvViMdzeeRGb9OJ923rr5fDjOh4iIbIHZAaigoAD5+fmIiooyaj9w4ABkMhkiIyMtVhxZp8LyGjy7fD8qajVwk8sw/aFOeDEmjON8iIjIZpg9D9D06dNx8eJFk/bLly9j+vTpFimKrFtq9jVU1GrQwd8NO18fhr8M68TwQ0RENsXsAJSVldXgXD+9e/dGVlaWRYoi65aWXQQAiO3WBgGeHORMRES2x+wA5OLigqtXr5q05+XlwcmJS4vZO0EQkHbmGgBgcGc/kashIiK6O2YHoEcffRSzZ89GSUmJoa24uBh/+9vf8Oijj1q0OLI+Z6+V42ppDVycpOgb4i12OURERHfF7C6bjz76CEOGDEFISAh69+4NAMjMzERgYCD+9a9/WbxAsi6p2YUAdLe7c9wPERHZKrMD0H333Ydjx45hzZo1OHr0KJRKJV544QVMmDChwTmByL6k1QegmE68/EVERLbrrgbtuLm5YerUqZauhaycWqPF/nO6AdCDGICIiMiG3fWo5aysLOTm5qK2ttao/fHHH7/nosg6ZV4sRkWtBj5ucnQL4iKxRERku+5qJuixY8fi+PHjkEgk0C8mr5/9V6PRWLZCshr6y1/RHX0hlXK2ZyIisl1m3wU2c+ZMhIWF4erVq3B1dcXJkyexe/duREZG4rfffmuBEslapJ3RBSBe/iIiIltndg/Qvn37sGPHDvj7+0MqlUIqlWLQoEFISkrCjBkzcOTIkZaok0RWVq02rPs1iPP/EBGRjTO7B0ij0cDd3R0A4OfnhytXrgAAQkJCcOrUKctWR1Zj/7nr0GgFhPm5oZ23q9jlEBER3ROze4AiIiJw7NgxdOjQAVFRUVi4cCHkcjmWLVuGDh06tESNZAX2nNHf/u4rciVERET3zuwA9Pbbb6OiogIAMH/+fIwaNQqDBw+Gr68v1q9fb/ECyTqkZuuWvxjUyV/kSoiIiO6d2QEoLi7O8LxDhw7IysrC9evX4e3tbbgTjOxLXkkVzl6rgFQCDOzIHiAiIrJ9Zo0Bqqurg5OTE06cOGHU7uPjw/Bjx/S3v/ds5wWVkrN9ExGR7TMrADk5OSEkJIRz/TiYPbz9nYiI7IzZd4G9/fbbmD17Nq5fv94S9ZCVEQQBaWfql7/g7e9ERGQnzB4D9Pnnn+PMmTNo27YtQkJC4ObmZrT98OHDFiuOxHfqahkKy2ugdJahd3svscshIiKyCLMD0JgxY1qgDLJW+vE/UR184OIkE7kaIiIiyzA7AL377rstUQdZKS5/QURE9sjsMUDkOGrqNDhwTjfWi+N/iIjInpjdAySVSpu85Z13iNmPI7nFqFJr4OfugvsDPcQuh4iIyGLMDkAbNmwweq1Wq3HkyBGsXr0ac+fOtVhhJD79+J9BnXw5zxMREdkVswPQE088YdL21FNPoXv37li/fj0mT55skcJIfGmG9b94+YuIiOyLxcYARUVFYfv27ZY6HYmspFKNY5eKAXD8DxER2R+LBKCqqiosXrwY7dq1s8TpyArsO1cIrQB09HdDkEopdjlEREQWZXYA8vb2ho+Pj+Hh7e0NDw8PrFixAosWLTK7gOTkZISFhUGhUKBv375ITU1tcv81a9agV69ecHV1RVBQEF544QUUFRUZtq9atQoSicTkUV1dbXZtjkx/+WtwZ67+TkRE9sfsMUCffPKJ0YBYqVQKf39/REVFwdvb26xzrV+/HomJiUhOTkZMTAyWLl2KESNGICsrC+3btzfZPy0tDc8//zw++eQTjB49GpcvX8a0adPw0ksvGQ3O9vT0xKlTp4yOVSgUZn5Sx6YfAM3xP0REZI/MDkAJCQkWe/OPP/4YkydPxksvvQQA+PTTT7FlyxakpKQgKSnJZP/9+/cjNDQUM2bMAACEhYXhz3/+MxYuXGi0n0QiQZs2bSxWp6O5eL0S54sqIZNKMKCDj9jlEBERWZzZl8BWrlyJb7/91qT922+/xerVq5t9ntraWmRkZCA2NtaoPTY2Fnv37m3wmOjoaFy6dAmbNm2CIAi4evUqvvvuOzz22GNG+5WXlyMkJATt2rXDqFGjcOTIkSZrqampQWlpqdHDkelXf38g2AseCmeRqyEiIrI8swPQBx98AD8/08siAQEBeP/995t9nsLCQmg0GgQGBhq1BwYGIj8/v8FjoqOjsWbNGowfPx5yuRxt2rSBl5cXFi9ebNina9euWLVqFTZu3Ii1a9dCoVAgJiYG2dnZjdaSlJQElUpleAQHBzf7c9gjLn9BRET2zuwAdOHCBYSFhZm0h4SEIDc31+wCbp9gTxCERifdy8rKwowZM/DOO+8gIyMDv/zyC3JycjBt2jTDPgMGDMDEiRPRq1cvDB48GP/+97/RpUsXo5B0u9mzZ6OkpMTwuHjxotmfw15otQL2ntUNKuft70REZK/MHgMUEBCAY8eOITQ01Kj96NGj8PX1bfZ5/Pz8IJPJTHp7CgoKTHqF9JKSkhATE4M33ngDANCzZ0+4ublh8ODBmD9/PoKCgkyOkUql6NevX5M9QC4uLnBxcWl27fYsK68U1ytq4e7ihAeCvcQuh4iIqEWY3QP0zDPPYMaMGdi5cyc0Gg00Gg127NiBmTNn4plnnmn2eeRyOfr27Ytt27YZtW/btg3R0dENHlNZWQmp1LhkmUwGQNdz1BBBEJCZmdlgOCJT+stfAzr4wFnGtXKJiMg+md0DNH/+fFy4cAEPP/wwnJx0h2u1Wjz//PNmjQECgFmzZiE+Ph6RkZEYOHAgli1bhtzcXMMlrdmzZ+Py5cv4+uuvAQCjR4/GlClTkJKSgri4OOTl5SExMRH9+/dH27ZtAQBz587FgAED0LlzZ5SWluLzzz9HZmYmvvjiC3M/qkPaw+UviIjIAZgdgORyOdavX4/58+cjMzMTSqUSPXr0QEhIiNlvPn78eBQVFWHevHnIy8tDREQENm3aZDhXXl6e0biihIQElJWVYcmSJXjttdfg5eWFhx56CAsWLDDsU1xcjKlTpyI/Px8qlQq9e/fG7t270b9/f7PrczTVag3Sc64DAAZz/A8REdkxidDYtSMHVlpaCpVKhZKSEnh6eopdTqvZc6YQz315AIGeLtg/+2GuAE9ERDbFnO9vswd5PPXUU/jggw9M2hctWoSnn37a3NORFUnN1t/+7s/wQ0REds3sALRr1y6TiQcBYPjw4di9e7dFiiJx6Mf/DOrc/Lv5iIiIbJHZAai8vBxyudyk3dnZ2eFnULZlNypqceJKCQAOgCYiIvtndgCKiIjA+vXrTdrXrVuHbt26WaQoan17zxZBEID7Az0Q4MGFY4mIyL6ZfRfYnDlz8OSTT+Ls2bN46KGHAAC//vorvvnmG3z33XcWL5BaR9qZawA4+zMRETkGswPQ448/jh9//BHvv/8+vvvuOyiVSvTq1Qs7duxwqDum7A3X/yIiIkdidgACgMcee8wwELq4uBhr1qxBYmIijh49Co1GY9ECqeVdKKrAxetVcJZJ0D/MR+xyiIiIWtxdr3WwY8cOTJw4EW3btsWSJUswcuRIHDp0yJK1USvR9/70bu8NN5e7ysREREQ2xaxvu0uXLmHVqlVYsWIFKioqMG7cOKjVanz//fccAG3D0urn/xnMy19EROQgmt0DNHLkSHTr1g1ZWVlYvHgxrly5gsWLF7dkbdQKNFoBe88WAQBiOACaiIgcRLN7gLZu3YoZM2bg5ZdfRufOnVuyJmpFJy6XoKRKDQ+FE3repxK7HCIiolbR7B6g1NRUlJWVITIyElFRUViyZAmuXbvWkrVRK9CP/xnYwRdOsrseEkZERGRTmv2NN3DgQCxfvhx5eXn485//jHXr1uG+++6DVqvFtm3bUFZW1pJ1UgsxjP/h5S8iInIgZv8vv6urK1588UWkpaXh+PHjeO211/DBBx8gICAAjz/+eEvUSC2kqlaDjAs3AHD5CyIiciz3dM3j/vvvx8KFC3Hp0iWsXbvWUjVRK0k/fx21Gi3u81IizM9N7HKIiIhajUUGfchkMowZMwYbN260xOmolaRl68ZwxXTyhUQiEbkaIiKi1sNRrw4s7Yzu9vdBnf1FroSIiKh1MQA5qGtlNfg9rxQAEN3RV+RqiIiIWhcDkIPae1Z391e3IE/4ubuIXA0REVHrYgByULz9nYiIHBkDkAMSBAF76idA5O3vRETkiBiAHNC5wgpcKamG3EmK/mE+YpdDRETU6hiAHJC+9ycyxBsKZ5nI1RAREbU+BiAHlFo//mcQx/8QEZGDYgByMHUaLfafrZ//h+N/iIjIQTEAOZijl0pQVlMHL1dndG+rErscIiIiUTAAORj9+J/ojr6QSbn8BREROSYGIAejn/9nUCcuf0FERI6LAciBVNTU4XDuDQAc/0NERI6NAciBHMgpQp1WQHsfV7T3dRW7HCIiItEwADmQtGzd3V+c/ZmIiBwdA5ADSTtzDQDX/yIiImIAchBXS6tx+mo5JBJgYAdfscshIiISFQOQg9Df/t7jPhW83eQiV0NERCQuBiAHob/9neN/iIiIGIAcgiAISKvvARrMAERERMQA5AiyC8pRUFYDFycp+oR4i10OERGR6BiAHID+8lf/MB8onGUiV0NERCQ+BiAHoL/8xdmfiYiIdBiA7Jxao8X+c7oJEAdx/h8iIiIADEB270huMSprNfB1kyO8jafY5RAREVkFBiA7p7/8Fd3JD1KpRORqiIiIrAMDkJ1Ly65f/oLjf4iIiAwYgOxYabUaRy+VAABiOP6HiIjIgAHIju0/WwSNVkAHPzfc56UUuxwiIiKrwQBkx/Trf3H5CyIiImMMQHYsVT//Dy9/ERERGWEAslNXiqtw7loFpBJgQAdfscshIiKyKgxAdkp/+3uvYC+olM4iV0NERGRdGIDs1B4uf0FERNQoBiA7pNUKDEBERERNYACyQ6eulqGwvBauchl6t/cWuxwiIiKrwwBkh9Kydb0/UWE+kDvxT0xERHQ7fjvaoVTO/0NERNQkBiA7U1OnQXpOEQBgcGd/kashIiKyTgxAdibjwg1Uq7Xw93BBl0B3scshIiKySqIHoOTkZISFhUGhUKBv375ITU1tcv81a9agV69ecHV1RVBQEF544QUUFRUZ7fP999+jW7ducHFxQbdu3bBhw4aW/AhW5da7vyQSicjVEBERWSdRA9D69euRmJiIt956C0eOHMHgwYMxYsQI5ObmNrh/Wloann/+eUyePBknT57Et99+i4MHD+Kll14y7LNv3z6MHz8e8fHxOHr0KOLj4zFu3DgcOHCgtT6WqPQDoDn+h4iIqHESQRAEsd48KioKffr0QUpKiqEtPDwcY8aMQVJSksn+H374IVJSUnD27FlD2+LFi7Fw4UJcvHgRADB+/HiUlpZi8+bNhn2GDx8Ob29vrF27tll1lZaWQqVSoaSkBJ6ennf78VpdSaUaD7y3FYIA7J/9MNqoFGKXRERE1GrM+f4WrQeotrYWGRkZiI2NNWqPjY3F3r17GzwmOjoaly5dwqZNmyAIAq5evYrvvvsOjz32mGGfffv2mZwzLi6u0XMCQE1NDUpLS40etmjv2UIIAtApwJ3hh4iIqAmiBaDCwkJoNBoEBgYatQcGBiI/P7/BY6Kjo7FmzRqMHz8ecrkcbdq0gZeXFxYvXmzYJz8/36xzAkBSUhJUKpXhERwcfA+fTDxpnP2ZiIioWUQfBH37QF1BEBodvJuVlYUZM2bgnXfeQUZGBn755Rfk5ORg2rRpd31OAJg9ezZKSkoMD/3lNFvDAERERNQ8TmK9sZ+fH2QymUnPTEFBgUkPjl5SUhJiYmLwxhtvAAB69uwJNzc3DB48GPPnz0dQUBDatGlj1jkBwMXFBS4uLvf4icR18XolLhRVQiaVYEBHX7HLISIismqi9QDJ5XL07dsX27ZtM2rftm0boqOjGzymsrISUqlxyTKZDICulwcABg4caHLOrVu3NnpOe6Hv/ekd7AV3F9FyLRERkU0Q9Zty1qxZiI+PR2RkJAYOHIhly5YhNzfXcElr9uzZuHz5Mr7++msAwOjRozFlyhSkpKQgLi4OeXl5SExMRP/+/dG2bVsAwMyZMzFkyBAsWLAATzzxBH766Sds374daWlpon3O1mC4/NWZl7+IiIjuRNQANH78eBQVFWHevHnIy8tDREQENm3ahJCQEABAXl6e0ZxACQkJKCsrw5IlS/Daa6/By8sLDz30EBYsWGDYJzo6GuvWrcPbb7+NOXPmoGPHjli/fj2ioqJa/fO1Fq1WwN76ADSYAYiIiOiORJ0HyFrZ2jxAJy6XYNTiNLi7OOHIO4/CWSb62HYiIqJWZxPzAJHlpNbP/jyggy/DDxERUTPw29IO3Fz/i3d/ERERNQcDkI2rVmuQfv46AGBQZ3+RqyEiIrINDEA27tD5G6it06KNpwId/d3ELoeIiMgmMADZuNQz1wDobn9varZrIiIiuokByMbt4fIXREREZmMAsmHXK2px8opu5foYBiAiIqJmYwCyYXvOFEIQgK5tPODvYdtrmREREbUmBiAbxstfREREd4cByEYJgmCYADGGy18QERGZhQHIRl0oqsTl4io4yySICvMRuxwiIiKbwgBko1LrL3/1ae8NV7moa9oSERHZHAYgG7Unm6u/ExER3S0GIBuk0QrYe7Z+/A8HQBMREZmNAcgGHb9cgtLqOngonNCznZfY5RAREdkcBiAblJatW/4iuqMvZFIuf0FERGQuBiAblKaf/4ervxMREd0VBiAbU1lbh4wLNwBwAkQiIqK7xQBkY9JzrkOtEXCflxKhvq5il0NERGSTGIBsTFr2zeUvJBKO/yEiIrobDEA25ub4H17+IiIiulsMQDbkWlkN/sgvA8D5f4iIiO4FA5AN0U9+2L2tJ3zc5CJXQ0REZLsYgGyIfvV3Xv4iIiK6NwxANkIQBOw5c3MANBEREd09BiAbcfZaBfJKqiF3kqJfqI/Y5RAREdk0BiAboe/96RfqDYWzTORqiIiIbBsDkI0wjP/pxOUviIiI7hUDkA1Qa7TYf64IAMf/EBERWQIDkA04dqkY5TV18HJ1Rve2nmKXQ0REZPMYgGyA/vJXTEc/SKVc/oKIiOheMQDZgD1c/oKIiMiiGICsXHlNHY7kFgPg+B8iIiJLYQCycgfOFaFOKyDE1xXBPq5il0NERGQXGICsnGH8D3t/iIiILIYByMrpx/8MZgAiIiKyGAYgK5ZfUo3sgnJIJMDAjr5il0NERGQ3GICsmL73p+d9Kni5ykWuhoiIyH4wAFmxtDMc/0NERNQSGICslCAIhgDE+X+IiIgsiwHISp2+Wo5rZTVQOEvRN8Rb7HKIiIjsCgOQldL3/vQP84WLk0zkaoiIiOwLA5CVSsu+BgAY1Il3fxEREVkaA5AVqq3T4kDOdQDAoE7+IldDRERkfxiArNCR3BuorNXAz12Orm08xC6HiIjI7jiJXQCZ0s//E93RD1KpRORqiIisk0ajgVqtFrsMamVyuRxS6b333zAAWaFU3v5ORNQoQRCQn5+P4uJisUshEUilUoSFhUEuv7cJghmArExptRpHLxYDAAZxAkQiIhP68BMQEABXV1dIJOwpdxRarRZXrlxBXl4e2rdvf09/ewYgK7PvbBG0AtDB3w1tvZRil0NEZFU0Go0h/Pj68i5ZR+Tv748rV66grq4Ozs7Od30eDoK2MmnZ9Ze/2PtDRGRCP+bH1dVV5EpILPpLXxqN5p7OwwBkZfQDoBmAiIgax8tejstSf3sGICtyubgK5worIJNKMKAju3aJiIhaCgOQFdlTf/mrVzsVPBV3f12TiIjsW2hoKD799NNm7//bb79BIpHwzrlbMABZkVRe/iIiskvDhg1DYmKixc538OBBTJ06tdn7R0dHIy8vDyqVymI1tARL/56awrvArIRWK2CvYf4fLn9BRORoBEGARqOBk9Odv5r9/c37npDL5WjTps3dlmaX2ANkJX7PL0VRRS1c5TI8EOwldjlERDZBEARU1taJ8hAEoVk1JiQkYNeuXfjss88gkUggkUhw/vx5w2WpLVu2IDIyEi4uLkhNTcXZs2fxxBNPIDAwEO7u7ujXrx+2b99udM7bL4FJJBJ8+eWXGDt2LFxdXdG5c2ds3LjRsP32S2CrVq2Cl5cXtmzZgvDwcLi7u2P48OHIy8szHFNXV4cZM2bAy8sLvr6+ePPNNzFp0iSMGTOm0c964cIFjB49Gt7e3nBzc0P37t2xadMmw/asrCyMHDkS7u7uCAwMRHx8PAoLC5v8PbUU0XuAkpOTsWjRIuTl5aF79+749NNPMXjw4Ab3TUhIwOrVq03au3XrhpMnTwLQ/VFfeOEFk32qqqqgUCgsW7wF6e/+GtDBF3In5lIiouaoUmvQ7Z0torx31rw4uMrv/DX62Wef4fTp04iIiMC8efMA6Hpw9F/u//u//4sPP/wQHTp0gJeXFy5duoSRI0di/vz5UCgUWL16NUaPHo1Tp06hffv2jb7P3LlzsXDhQixatAiLFy/Gc889hwsXLsDHx6fB/SsrK/Hhhx/iX//6F6RSKSZOnIjXX38da9asAQAsWLAAa9aswcqVKxEeHo7PPvsMP/74Ix588MFGa5g+fTpqa2uxe/duuLm5ISsrC+7u7gCAvLw8DB06FFOmTMHHH3+MqqoqvPnmmxg3bhx27NjR6O+ppYgagNavX4/ExEQkJycjJiYGS5cuxYgRI5CVldXgH/mzzz7DBx98YHhdV1eHXr164emnnzbaz9PTE6dOnTJqs+bwAwCp9QOgYzj+h4jIrqhUKsjlcri6ujZ4GWrevHl49NFHDa99fX3Rq1cvw+v58+djw4YN2LhxI1555ZVG3ychIQETJkwAALz//vtYvHgx0tPTMXz48Ab3V6vV+Oc//4mOHTsCAF555RVD8ACAxYsXY/bs2Rg7diwAYMmSJUa9OQ3Jzc3Fk08+iR49egAAOnToYNiWkpKCPn364P333ze0rVixAsHBwTh9+jS6dOnS5O/J0kQNQB9//DEmT56Ml156CQDw6aefYsuWLUhJSUFSUpLJ/iqVymgA148//ogbN26Y9PhIJBKbutZZrdbg4PnrAIDBXP+LiKjZlM4yZM2LE+29LSEyMtLodUVFBebOnYv//ve/hhmPq6qqkJub2+R5evbsaXju5uYGDw8PFBQUNLq/q6urIfwAQFBQkGH/kpISXL16Ff379zdsl8lk6Nu3L7RabaPnnDFjBl5++WVs3boVjzzyCJ588klDXRkZGdi5c6ehR+hWZ8+eRZcuXZr8fJYmWgCqra1FRkYG/vrXvxq1x8bGYu/evc06x1dffYVHHnkEISEhRu3l5eUICQmBRqPBAw88gPfeew+9e/du9Dw1NTWoqakxvC4tLTXjk9y7wxduoFqtRYCHCzoHmP6DQUREDZNIJM26DGXN3NzcjF6/8cYb2LJlCz788EN06tQJSqUSTz31FGpra5s8z+3LQkgkkibDSkP73z6u6fZJB+807umll15CXFwcfv75Z2zduhVJSUn46KOP8Oqrr0Kr1WL06NFYsGCByXFBQUFNnrcliDbYpLCwEBqNBoGBgUbtgYGByM/Pv+PxeXl52Lx5s6H3SK9r165YtWoVNm7ciLVr10KhUCAmJgbZ2dmNnispKcnQu6RSqRAcHHx3H+oupd1y+ztnNyUisj9yubzZSzekpqYiISEBY8eORY8ePdCmTZsWHQzcEJVKhcDAQKSnpxvaNBoNjhw5csdjg4ODMW3aNPzwww947bXXsHz5cgBAnz59cPLkSYSGhqJTp05GD30INOf3dK9EH23bULpsTgjQj2C/fTT6gAEDMHHiRPTq1QuDBw/Gv//9b3Tp0gWLFy9u9FyzZ89GSUmJ4XHx4sW7+ix3Sx+AOP6HiMg+hYaG4sCBAzh//jwKCwub7Jnp1KkTfvjhB2RmZuLo0aN49tlnm9y/pbz66qtISkrCTz/9hFOnTmHmzJm4ceNGk9/RiYmJ2LJlC3JycnD48GHs2LED4eHhAHQDpK9fv44JEyYgPT0d586dw9atW/Hiiy8aQo85v6d7JVoA8vPzg0wmM+ntKSgoMOkVup0gCFixYgXi4+MNi6I1RiqVol+/fk32ALm4uMDT09Po0VqKK2tx/HIJAGAQx/8QEdml119/HTKZDN26dYO/v3+T43k++eQTeHt7Izo6GqNHj0ZcXBz69OnTitXqvPnmm5gwYQKef/55DBw4EO7u7oiLi2vypiKNRoPp06cjPDwcw4cPx/3334/k5GQAQNu2bbFnzx5oNBrExcUhIiICM2fOhEqlglSqiyPm/J7ulURo7kQGLSAqKgp9+/Y1/HIA3S3tTzzxRIODoPV+++03PPjggzh+/DgiIiKafA9BENC/f3/06NEDK1asaFZdpaWlUKlUKCkpafEwtOl4Hv6y5jA6B7hj26yhLfpeRES2rrq6Gjk5OQgLC7P6u3vtjVarRXh4OMaNG4f33ntPtDqa+mfAnO9vUUeOzZo1C/Hx8YiMjMTAgQOxbNky5ObmYtq0aQB0l6YuX76Mr7/+2ui4r776ClFRUQ2Gn7lz52LAgAHo3LkzSktL8fnnnyMzMxNffPFFq3wmcxnG/7D3h4iIrMiFCxewdetWDB06FDU1NViyZAlycnLw7LPPil2aRYgagMaPH4+ioiLMmzcPeXl5iIiIwKZNmwx3deXl5Zl0f5WUlOD777/HZ5991uA5i4uLMXXqVOTn50OlUqF3797YvXu30a181iQtm+t/ERGR9ZFKpVi1ahVef/11CIKAiIgIbN++3TCmx9aJegnMWrXWJbDcokoMWbQTTlIJMt+NhbuLbd/KSUTU0ngJjCx1CUz0u8Acmf7yV5/23gw/RERErYgBSER7ePs7ERGRKBiARKLRCthzlgOgiYiIxMAAJJKTV0pQXKmGh4sTerVT3fkAIiIishgGIJHox/8M6OgLJxn/DERERK2J37wi4e3vRERE4mEAEkFVrQaHzt8AwPE/REREYmAAEsHB89dRq9EiSKVABz83scshIqIWNmzYMCQmJlr0nAkJCSYLglva+fPnIZFIkJmZ2aLvIwYGIBHob38f1MmvyVV1iYiIqGUwAIkgNZu3vxMRWYQgALUV4jyauZBCQkICdu3ahc8++wwSiQQSiQTnz58HAGRlZWHkyJFwd3dHYGAg4uPjUVhYaDj2u+++Q48ePaBUKuHr64tHHnkEFRUV+Pvf/47Vq1fjp59+Mpzzt99+a/D9GzuH3sqVKxEeHg6FQoGuXbsaLVAeFhYGAOjduzckEgmGDRtm3t/HinH64VZWVF6DrLxSAEB0RwYgIqJ7oq4E3m8rznv/7Qogv/Mwhs8++wynT59GREQE5s2bBwDw9/dHXl4ehg4diilTpuDjjz9GVVUV3nzzTYwbNw47duxAXl4eJkyYgIULF2Ls2LEoKytDamoqBEHA66+/jt9//x2lpaVYuXIlAMDHx8fkvZs6BwAsX74c7777LpYsWYLevXvjyJEjmDJlCtzc3DBp0iSkp6ejf//+2L59O7p37w65XG7BX6C4GIBa2Z6zRQCArm084O/hInI1RETU0lQqFeRyOVxdXdGmTRtDe0pKCvr06YP333/f0LZixQoEBwfj9OnTKC8vR11dHf70pz8ZFgnv0aOHYV+lUomamhqjc94uLy+vyXO89957+Oijj/CnP/0JgK7HJysrC0uXLsWkSZPg7+8PAPD19W3yfWwRA1Ar21N/+WswL38REd07Z1ddT4xY730PMjIysHPnTri7u5tsO3v2LGJjY/Hwww+jR48eiIuLQ2xsLJ566il4e3s3+z169erV6DmuXbuGixcvYvLkyZgyZYrhmLq6OqhU9j9BLwNQKxIEwTABItf/IiKyAImkWZehrJFWq8Xo0aOxYMECk21BQUGQyWTYtm0b9u7di61bt2Lx4sV46623cODAAcPYnDtp6hyurroAt3z5ckRFRZkcZ+84CLoVnS+qxOXiKshlUvQPM71WS0RE9kkul0Oj0Ri19enTBydPnkRoaCg6depk9HBz04U6iUSCmJgYzJ07F0eOHIFcLseGDRsaPWdDGjtHYGAg7rvvPpw7d87k/fUBSz/mpznvY2vYA9SKLt2ohK+bHJ0D3eEq56+eiMhRhIaG4sCBAzh//jzc3d3h4+OD6dOnY/ny5ZgwYQLeeOMN+Pn54cyZM1i3bh2WL1+OQ4cO4ddff0VsbCwCAgJw4MABXLt2DeHh4YZzbtmyBadOnYKvry9UKhWcnZ2N3vfAgQNNnuPvf/87ZsyYAU9PT4wYMQI1NTU4dOgQbty4gVmzZiEgIABKpRK//PIL2rVrB4VCYT+XxwQyUVJSIgAQSkpKLH5ujUYrXC+vsfh5iYgcQVVVlZCVlSVUVVWJXYpZTp06JQwYMEBQKpUCACEnJ0cQBEE4ffq0MHbsWMHLy0tQKpVC165dhcTEREGr1QpZWVlCXFyc4O/vL7i4uAhdunQRFi9ebDhnQUGB8Oijjwru7u4CAGHnzp0m73uncwiCIKxZs0Z44IEHBLlcLnh7ewtDhgwRfvjhB8P25cuXC8HBwYJUKhWGDh3aEr8eszT1z4A5398SQWjmRAYOpLS0FCqVCiUlJfD09BS7HCIiqlddXY2cnByEhYVBoVCIXQ6JoKl/Bsz5/uYYICIiInI4DEBERETkcBiAiIiIyOEwABEREZHDYQAiIiKbw/t3HJel/vYMQEREZDP089xUVlaKXAmJpba2FsC9z1bN2fiIiMhmyGQyeHl5oaCgAADg6uoKiUQiclXUWrRaLa5duwZXV1c4Od1bhGEAIiIim6JflVwfgsixSKVStG/f/p6DLwMQERHZFIlEgqCgIAQEBECtVotdDrUyuVwOqfTeR/AwABERkU2SyWQOsWo5tQwOgiYiIiKHwwBEREREDocBiIiIiBwOxwA1QD/JUmlpqciVEBERUXPpv7ebM1kiA1ADysrKAADBwcEiV0JERETmKisrg0qlanIficD5xE1otVpcuXIFHh4eFp9gq7S0FMHBwbh48SI8PT0tem4yH/8e1oV/D+vCv4f14d+kaYIgoKysDG3btr3jrfLsAWqAVCpFu3btWvQ9PD09+Q+vFeHfw7rw72Fd+PewPvybNO5OPT96HARNREREDocBiIiIiBwOA1Arc3FxwbvvvgsXFxexSyHw72Ft+PewLvx7WB/+TSyHg6CJiIjI4bAHiIiIiBwOAxARERE5HAYgIiIicjgMQERERORwGIBaUXJyMsLCwqBQKNC3b1+kpqaKXZLDSkpKQr9+/eDh4YGAgACMGTMGp06dErssgu5vI5FIkJiYKHYpDu3y5cuYOHEifH194erqigceeAAZGRlil+WQ6urq8PbbbyMsLAxKpRIdOnTAvHnzoNVqxS7NpjEAtZL169cjMTERb731Fo4cOYLBgwdjxIgRyM3NFbs0h7Rr1y5Mnz4d+/fvx7Zt21BXV4fY2FhUVFSIXZpDO3jwIJYtW4aePXuKXYpDu3HjBmJiYuDs7IzNmzcjKysLH330Eby8vMQuzSEtWLAA//znP7FkyRL8/vvvWLhwIRYtWoTFixeLXZpN423wrSQqKgp9+vRBSkqKoS08PBxjxoxBUlKSiJURAFy7dg0BAQHYtWsXhgwZInY5Dqm8vBx9+vRBcnIy5s+fjwceeACffvqp2GU5pL/+9a/Ys2cPe6mtxKhRoxAYGIivvvrK0Pbkk0/C1dUV//rXv0SszLaxB6gV1NbWIiMjA7GxsUbtsbGx2Lt3r0hV0a1KSkoAAD4+PiJX4rimT5+Oxx57DI888ojYpTi8jRs3IjIyEk8//TQCAgLQu3dvLF++XOyyHNagQYPw66+/4vTp0wCAo0ePIi0tDSNHjhS5MtvGxVBbQWFhITQaDQIDA43aAwMDkZ+fL1JVpCcIAmbNmoVBgwYhIiJC7HIc0rp163D48GEcPHhQ7FIIwLlz55CSkoJZs2bhb3/7G9LT0zFjxgy4uLjg+eefF7s8h/Pmm2+ipKQEXbt2hUwmg0ajwT/+8Q9MmDBB7NJsGgNQK5JIJEavBUEwaaPW98orr+DYsWNIS0sTuxSHdPHiRcycORNbt26FQqEQuxwCoNVqERkZiffffx8A0Lt3b5w8eRIpKSkMQCJYv349/u///g/ffPMNunfvjszMTCQmJqJt27aYNGmS2OXZLAagVuDn5weZTGbS21NQUGDSK0St69VXX8XGjRuxe/dutGvXTuxyHFJGRgYKCgrQt29fQ5tGo8Hu3buxZMkS1NTUQCaTiVih4wkKCkK3bt2M2sLDw/H999+LVJFje+ONN/DXv/4VzzzzDACgR48euHDhApKSkhiA7gHHALUCuVyOvn37Ytu2bUbt27ZtQ3R0tEhVOTZBEPDKK6/ghx9+wI4dOxAWFiZ2SQ7r4YcfxvHjx5GZmWl4REZG4rnnnkNmZibDjwhiYmJMpoU4ffo0QkJCRKrIsVVWVkIqNf66lslkvA3+HrEHqJXMmjUL8fHxiIyMxMCBA7Fs2TLk5uZi2rRpYpfmkKZPn45vvvkGP/30Ezw8PAy9cyqVCkqlUuTqHIuHh4fJ2Cs3Nzf4+vpyTJZI/ud//gfR0dF4//33MW7cOKSnp2PZsmVYtmyZ2KU5pNGjR+Mf//gH2rdvj+7du+PIkSP4+OOP8eKLL4pdmk3jbfCtKDk5GQsXLkReXh4iIiLwySef8JZrkTQ29mrlypVISEho3WLIxLBhw3gbvMj++9//Yvbs2cjOzkZYWBhmzZqFKVOmiF2WQyorK8OcOXOwYcMGFBQUoG3btpgwYQLeeecdyOVyscuzWQxARERE5HA4BoiIiIgcDgMQERERORwGICIiInI4DEBERETkcBiAiIiIyOEwABEREZHDYQAiIiIih8MARERERA6HAYiIbI5EIsGPP/4odhlm+e233yCRSFBcXCx2KUQEBiAiMkNCQgIkEonJY/jw4WKXdkfDhg2DRCLBunXrjNo//fRThIaGilMUEYmGAYiIzDJ8+HDk5eUZPdauXSt2Wc2iUCjw9ttvQ61Wi12KxdTW1opdApFNYgAiIrO4uLigTZs2Rg9vb2/DdolEgpSUFIwYMQJKpRJhYWH49ttvjc5x/PhxPPTQQ1AqlfD19cXUqVNRXl5utM+KFSvQvXt3uLi4ICgoCK+88orR9sLCQowdOxaurq7o3LkzNm7ceMfaJ0yYgJKSEixfvrzRfRISEjBmzBijtsTERAwbNszwetiwYXj11VeRmJgIb29vBAYGYtmyZaioqMALL7wADw8PdOzYEZs3bzY5/549e9CrVy8oFApERUXh+PHjRtv37t2LIUOGQKlUIjg4GDNmzEBFRYVhe2hoKObPn4+EhASoVCouUEp0lxiAiMji5syZgyeffBJHjx7FxIkTMWHCBPz+++8AgMrKSgwfPhze3t44ePAgvv32W2zfvt0o4KSkpGD69OmYOnUqjh8/jo0bN6JTp05G7zF37lyMGzcOx44dw8iRI/Hcc8/h+vXrTdbl6emJv/3tb5g3b55RqLgbq1evhp+fH9LT0/Hqq6/i5ZdfxtNPP43o6GgcPnwYcXFxiI+PR2VlpdFxb7zxBj788EMcPHgQAQEBePzxxw09UsePH0dcXBz+9Kc/4dixY1i/fj3S0tJMwt+iRYsQERGBjIwMzJkz554+B5HDEoiImmnSpEmCTCYT3NzcjB7z5s0z7ANAmDZtmtFxUVFRwssvvywIgiAsW7ZM8Pb2FsrLyw3bf/75Z0EqlQr5+fmCIAhC27ZthbfeeqvROgAIb7/9tuF1eXm5IJFIhM2bNzd6zNChQ4WZM2cK1dXVQkhIiKHmTz75RAgJCTH6jE888YTRsTNnzhSGDh1qdK5BgwYZXtfV1Qlubm5CfHy8oS0vL08AIOzbt08QBEHYuXOnAEBYt26dYZ+ioiJBqVQK69evFwRBEOLj44WpU6cavXdqaqoglUqFqqoqQRAEISQkRBgzZkyjn5OImsdJ3PhFRLbmwQcfREpKilGbj4+P0euBAweavM7MzAQA/P777+jVqxfc3NwM22NiYqDVanHq1ClIJBJcuXIFDz/8cJN19OzZ0/Dczc0NHh4eKCgouGP9Li4umDdvHl555RW8/PLLd9y/Oe8vk8ng6+uLHj16GNoCAwMBwKSmW383Pj4+uP/++w29YxkZGThz5gzWrFlj2EcQBGi1WuTk5CA8PBwAEBkZedd1E5EOAxARmcXNzc3kclRzSCQSALovdP3zhvZRKpXNOp+zs7PJsVqttlnHTpw4ER9++CHmz59vcgeYVCqFIAhGbQ0Nmm7o/W9t03/G5tR0675//vOfMWPGDJN92rdvb3h+a3gkorvDMUBEZHH79+83ed21a1cAQLdu3ZCZmWk0BmfPnj2QSqXo0qULPDw8EBoail9//bXF6pNKpUhKSkJKSgrOnz9vtM3f3x95eXlGbfreK0u49Xdz48YNnD592vC76dOnD06ePIlOnTqZPORyucVqICIGICIyU01NDfLz840ehYWFRvt8++23WLFiBU6fPo13330X6enphoG8zz33HBQKBSZNmoQTJ05g586dePXVVxEfH2+4bPT3v/8dH330ET7//HNkZ2fj8OHDWLx4sUU/x2OPPYaoqCgsXbrUqP2hhx7CoUOH8PXXXyM7OxvvvvsuTpw4YbH3nTdvHn799VecOHECCQkJ8PPzM9x19uabb2Lfvn2YPn06MjMzkZ2djY0bN+LVV1+12PsTkQ4DEBGZ5ZdffkFQUJDRY9CgQUb7zJ07F+vWrUPPnj2xevVqrFmzBt26dQMAuLq6YsuWLbh+/Tr69euHp556Cg8//DCWLFliOH7SpEn49NNPkZycjO7du2PUqFHIzs62+GdZsGABqqurjdri4uIwZ84c/O///i/69euHsrIyPP/88xZ7zw8++AAzZ85E3759kZeXh40bNxp6d3r27Ildu3YhOzsbgwcPRu/evTFnzhwEBQVZ7P2JSEci3H6xm4joHkgkEmzYsMFkLh0iImvCHiAiIiJyOAxARERE5HB4GzwRWRSvqhORLWAPEBERETkcBiAiIiJyOAxARERE5HAYgIiIiMjhMAARERGRw2EAIiIiIofDAEREREQOhwGIiIiIHM7/A1Mp4obG6vukAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(training_history.history['accuracy'], label='training set')\n",
    "plt.plot(training_history.history['val_accuracy'], label='test set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model accuracy\n",
    "\n",
    "We need to compare the accuracy of our model on **training** set and on **test** set. We expect our model to perform similarly on both sets. If the performance on a test set will be poor comparing to a training set it would be an indicator for us that the model is overfitted and we have a \"high variance\" issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_loss, train_accuracy = model.evaluate(x_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.042629458010196686\n",
      "Training accuracy:  0.9878833293914795\n"
     ]
    }
   ],
   "source": [
    "print('Training loss: ', train_loss)\n",
    "print('Training accuracy: ', train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "validation_loss, validation_accuracy = model.evaluate(x_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.058686986565589905\n",
      "Validation accuracy:  0.9839000105857849\n"
     ]
    }
   ],
   "source": [
    "print('Validation loss: ', validation_loss)\n",
    "print('Validation accuracy: ', validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "We will save the entire model to a `HDF5` file. The `.h5` extension of the file indicates that the model shuold be saved in Keras format as HDF5 file. To use this model on the front-end we will convert it (later in this notebook) to Javascript understandable format (`tfjs_layers_model` with .json and .bin files) using [tensorflowjs_converter](https://www.tensorflow.org/js/tutorials/conversion/import_saved_model) as it is specified in the [main README](https://github.com/trekhleb/machine-learning-experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'digits_recognition_cnn.h5'\n",
    "model.save(model_name, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model (do predictions)\n",
    "\n",
    "To use the model that we've just trained for digits recognition we need to call `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reduced = x_test[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, input shapes: [[None, 28, 28, 1]], output shape: [None, 28, 28, 1]\n",
      "Layer name: conv1, layer type: Conv2D, input shapes: [[None, 28, 28, 1]], output shape: [None, 24, 24, 8]\n",
      "Layer name: pool_1, layer type: MaxPooling2D, input shapes: [[None, 24, 24, 8]], output shape: [None, 12, 12, 8]\n",
      "Layer name: conv2, layer type: Conv2D, input shapes: [[None, 12, 12, 8]], output shape: [None, 8, 8, 16]\n",
      "Layer name: pool_2, layer type: MaxPooling2D, input shapes: [[None, 8, 8, 16]], output shape: [None, 4, 4, 16]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 4, 4, 16]], output shape: [None, 256]\n",
      "Layer name: dense1, layer type: Dense, input shapes: [[None, 256]], output shape: [None, 10]\n",
      "Layer name: output, layer type: Dense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, input shapes: [[None, 28, 28, 1]], output shape: [None, 28, 28, 1]\n",
      "Layer name: conv1, layer type: Conv2D, input shapes: [[None, 28, 28, 1]], output shape: [None, 24, 24, 8]\n",
      "Layer name: pool_1, layer type: MaxPooling2D, input shapes: [[None, 24, 24, 8]], output shape: [None, 12, 12, 8]\n",
      "Layer name: conv2, layer type: Conv2D, input shapes: [[None, 12, 12, 8]], output shape: [None, 8, 8, 16]\n",
      "Layer name: pool_2, layer type: MaxPooling2D, input shapes: [[None, 8, 8, 16]], output shape: [None, 4, 4, 16]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 4, 4, 16]], output shape: [None, 256]\n",
      "Layer name: dense1, layer type: Dense, input shapes: [[None, 256]], output shape: [None, 10]\n",
      "Layer name: output, layer type: Dense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "WARNING: Cannot use \"Latency\" model strategy for conv1 layer. Switching to \"Resource\" strategy.\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "\n",
    "# config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "# print(\"-----------------------------------\")\n",
    "# print(\"Configuration\")\n",
    "# # plotting.print_dict(config)\n",
    "# print(\"-----------------------------------\")\n",
    "# hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "#     model, hls_config=config, output_dir='model_1/hls4ml_prj', part='xczu7ev-ffvc1156-2-e'\n",
    "# )\n",
    "\n",
    "\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "hls_config['LayerName']['conv1']['Strategy'] = 'Latency'\n",
    "hls_config['LayerName']['conv2']['Strategy'] = 'Latency'\n",
    "\n",
    "hls_config['LayerName']['pool_1']['Strategy'] = 'Latency'\n",
    "\n",
    "\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "hls_config['LayerName']['output']['Strategy'] = 'Stable'\n",
    "#plotting.print_dict(hls_config_aq)\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "cfg['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg['HLSConfig'] = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'normal_cnn/'\n",
    "cfg['XilinxPart'] = 'zcu104'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg )\n",
    "hls_model.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_test = np.ascontiguousarray(x_test_normalized)  \n",
    "y_predict_hls4ml = hls_model.predict( X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"-------------------------Start--------------------------\")\n",
    "# hls_model.compile()\n",
    "# X_test = np.ascontiguousarray(x_test_normalized)\n",
    "# y_hls = hls_model.predict(X_test)\n",
    "# print(\"--------------------------End-------------------------\")\n",
    "# print(\"compilation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /home/ayush/vivado_2019/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/home/ayush/vivado_2019/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'ayush' on host 'binodssd' (Linux_x86_64 version 5.19.0-45-generic) on Wed Jun 21 01:06:21 IST 2023\n",
      "INFO: [HLS 200-10] On os Ubuntu 22.04.2 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/ayush/lasthls4ml/normal_cnn'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Opening project '/home/ayush/lasthls4ml/normal_cnn/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Opening solution '/home/ayush/lasthls4ml/normal_cnn/myproject_prj/solution1'.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "INFO: [HLS 200-10] Setting target device to 'xcku115-flvb2104-2-i'\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:39:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:39:76\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:51:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:51:76\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:64:67\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:64:71\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:72:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:72:77\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 8 issue(s) in file firmware/myproject.cpp\n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:16 ; elapsed = 00:00:17 . Memory (MB): peak = 939.125 ; gain = 524.027 ; free physical = 1335 ; free virtual = 104807\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:16 ; elapsed = 00:00:17 . Memory (MB): peak = 939.125 ; gain = 524.027 ; free physical = 1335 ; free virtual = 104807\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:244) in function 'void nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>(FORWARD_REFERENCE const&, ap_shift_reg<FORWARD_REFERENCE::value_type, FORWARD_REFERENCE::in_width> (*) [FORWARD_REFERENCE::n_chan], FORWARD_REFERENCE::value_type*)' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:244) in function 'void nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>(FORWARD_REFERENCE const&, ap_shift_reg<FORWARD_REFERENCE::value_type, FORWARD_REFERENCE::in_width> (*) [FORWARD_REFERENCE::n_chan], FORWARD_REFERENCE::value_type*)' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:233) in function 'void nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>(FORWARD_REFERENCE const&, ap_shift_reg<FORWARD_REFERENCE::value_type, FORWARD_REFERENCE::in_width> (*) [FORWARD_REFERENCE::n_chan], FORWARD_REFERENCE::value_type*)' completely with a factor of 1.\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:237).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:252).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_resource_rf_leq_nin<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_resource.h:56).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[].1' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:305).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[].1' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:203).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:54).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:52).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:52).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:51).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, config2>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:286).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:294).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:66->firmware/nnet_utils/nnet_conv_stream.h:294).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv2d_stream.h:87).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_conv_stream.h:237).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:237).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_conv_stream.h:252).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce_pool<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, config4>' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:204).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' into 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:247).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:188->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_common.h:43->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_common.h:43->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config5>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:252).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:305).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:203).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>' (firmware/nnet_utils/nnet_dense_stream.h:44).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:54).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:52).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:52).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:51).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config5>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:286).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:294).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_latency.h:66->firmware/nnet_utils/nnet_conv_stream.h:294).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv2d_stream.h:87).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[].1' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_conv_stream.h:237).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_conv_stream.h:252).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce_pool<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, config7>' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:204).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' into 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:247).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:188->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_common.h:43->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_common.h:43->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_pooling_stream.h:21->firmware/nnet_utils/nnet_pooling_stream.h:204->firmware/nnet_utils/nnet_pooling_stream.h:247->firmware/nnet_utils/nnet_pooling_stream.h:257).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_resource_rf_leq_nin<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' into 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_resource.h:253).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' into 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_resource.h:79->firmware/nnet_utils/nnet_dense_resource.h:253).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_resource<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_stream.h:21).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:244).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:203).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config11>' (firmware/nnet_utils/nnet_dense_stream.h:60).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config11>' (firmware/nnet_utils/nnet_dense_stream.h:44).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' (firmware/nnet_utils/nnet_activation_stream.h:54).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' (firmware/nnet_utils/nnet_activation_stream.h:52).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' (firmware/nnet_utils/nnet_activation_stream.h:52).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' (firmware/nnet_utils/nnet_activation_stream.h:51).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>' (firmware/nnet_utils/nnet_dense_stream.h:60).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' (firmware/nnet_utils/nnet_dense_stream.h:19).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' (firmware/nnet_utils/nnet_dense_latency.h:66->firmware/nnet_utils/nnet_dense_stream.h:19).\n",
      "INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:18 ; elapsed = 00:00:20 . Memory (MB): peak = 941.051 ; gain = 525.953 ; free physical = 1268 ; free virtual = 104744\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:209) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config12>' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:224) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:232) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config12>' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:235) automatically.\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:19 ; elapsed = 00:00:20 . Memory (MB): peak = 941.051 ; gain = 525.953 ; free physical = 1718 ; free virtual = 105197\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_pack.data.V' (firmware/nnet_utils/nnet_activation_stream.h:237) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:45) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:45) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:45) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_pooling_stream.h:184) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_pooling_stream.h:184) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_dense_stream.h:55) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_dense_stream.h:55) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:282) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:282) into a 128-bit variable.\n",
      "WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (firmware/nnet_utils/nnet_conv_stream.h:195) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>'.\n",
      "WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (firmware/nnet_utils/nnet_conv_stream.h:195) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>'.\n",
      "WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (firmware/nnet_utils/nnet_dense_resource.h:44) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>'.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:193:47).\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:41) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:41) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' (firmware/nnet_utils/nnet_activation_stream.h:41:42).\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_pooling_stream.h:243) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' for pipelining.\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:193) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_pooling_stream.h:243) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' for pipelining.\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:193) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'DataPrepare' (firmware/nnet_utils/nnet_dense_stream.h:36) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:81) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' for pipelining.\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:194) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:81) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' for pipelining.\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:194) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' (firmware/nnet_utils/nnet_dense_latency.h:17:48).\n",
      "INFO: [XFORM 203-502] Unrolling small iteration loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:43) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' automatically.\n",
      "INFO: [HLS 200-489] Unrolling loop 'SoftmaxArrayPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:201) in function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-2' (firmware/nnet_utils/nnet_activation_stream.h:213) in function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-3' (firmware/nnet_utils/nnet_activation_stream.h:222) in function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'SoftmaxInvPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:241) in function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:49) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:233) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:241) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:194) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:197) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:210) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:213) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:193) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-489] Unrolling loop 'PoolLoop' (firmware/nnet_utils/nnet_pooling_stream.h:198) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:233) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:241) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:194) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:197) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:210) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:213) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:193) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'PoolLoop' (firmware/nnet_utils/nnet_pooling_stream.h:198) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'DataPack' (firmware/nnet_utils/nnet_dense_stream.h:42) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResPack' (firmware/nnet_utils/nnet_dense_stream.h:58) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'DataPack' (firmware/nnet_utils/nnet_dense_stream.h:42) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResPack' (firmware/nnet_utils/nnet_dense_stream.h:58) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1.1.1' (firmware/nnet_utils/nnet_conv_stream.h:276) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 199.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:233) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:241) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:244) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:194) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:197) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 5.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:210) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 5.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:213) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 200.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 200.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:303) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:244) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:194) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:197) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 5.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:210) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 5.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 25.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 25.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:303) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'InitAccum' (firmware/nnet_utils/nnet_dense_resource.h:37) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReuseLoop' (firmware/nnet_utils/nnet_dense_resource.h:43) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'MultLoop' (firmware/nnet_utils/nnet_dense_resource.h:52) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 2560.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_resource.h:77) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' completely with a factor of 10.\n",
      "INFO: [XFORM 203-102] Partitioning array 'd_xi_xmax.V' (firmware/nnet_utils/nnet_activation_stream.h:212) automatically.\n",
      "INFO: [XFORM 203-131] Reshaping array 'w9.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.3' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.2' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.1' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer11_out.V.data.V' (firmware/myproject.cpp:70) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer12_out.V.data.V' (firmware/myproject.cpp:8) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer2_out.V.data.V' (firmware/myproject.cpp:37) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer3_out.V.data.V' (firmware/myproject.cpp:41) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer5_out.V.data.V' (firmware/myproject.cpp:49) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer6_out.V.data.V' (firmware/myproject.cpp:53) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer9_out.V.data.V' (firmware/myproject.cpp:62) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer10_out.V.data.V' (firmware/myproject.cpp:66) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer4_out.V.data.V' (firmware/myproject.cpp:45) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer7_out.V.data.V' (firmware/myproject.cpp:57) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'conv1_input.V.data.V' (firmware/myproject.cpp:7) .\n",
      "INFO: [XFORM 203-101] Partitioning array 'data_array.V' (firmware/nnet_utils/nnet_activation_stream.h:193) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'exp_res.V' (firmware/nnet_utils/nnet_activation_stream.h:219) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.3' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V.5' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'pool_window.V' (firmware/nnet_utils/nnet_pooling_stream.h:178) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.3'  in dimension 1 completely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.2' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V.6' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'pool_window.V' (firmware/nnet_utils/nnet_pooling_stream.h:178) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.2'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 1 completely.\n",
      "WARNING: [XFORM 203-104] Completely partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:29) accessed through non-constant indices on dimension 1 (firmware/nnet_utils/nnet_dense_stream.h:44:39), which may result in long runtime and suboptimal QoR due to large multiplexers. Please consider wrapping the array access into a function or using a register file core instead.\n",
      "INFO: [XFORM 203-101] Partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:29) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:29) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.1' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V.9' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.1'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b5.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:17) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:18) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b2.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:17) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:18) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b11.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:17) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:18) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b9.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_resource.h:33) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer11_out.V.data.V' (firmware/myproject.cpp:70) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer12_out.V.data.V' (firmware/myproject.cpp:8) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer2_out.V.data.V' (firmware/myproject.cpp:37) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer3_out.V.data.V' (firmware/myproject.cpp:41) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer5_out.V.data.V' (firmware/myproject.cpp:49) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer6_out.V.data.V' (firmware/myproject.cpp:53) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer9_out.V.data.V' (firmware/myproject.cpp:62) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer10_out.V.data.V' (firmware/myproject.cpp:66) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer4_out.V.data.V' (firmware/myproject.cpp:45) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer7_out.V.data.V' (firmware/myproject.cpp:57) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'conv1_input.V.data.V' (firmware/myproject.cpp:7) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.3'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.2'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.1'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:229) in dimension 2 completely.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:209) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config12>' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:224) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config12>' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:235) automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-712] Applying dataflow to function 'myproject', detected/extracted 10 process function(s): \n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>'\n",
      "\t 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>'\n",
      "\t 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>'\n",
      "\t 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>'\n",
      "\t 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config11>'\n",
      "\t 'nnet::softmax<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_activation_stream.h:248:1) in function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>'... converting 41 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:41:61) to (firmware/nnet_utils/nnet_activation_stream.h:41:55) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>'... converting 25 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:41:61) to (firmware/nnet_utils/nnet_activation_stream.h:41:55) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>'... converting 49 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_activation_stream.h:59:1) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>'... converting 31 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:45:44) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:43:16) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:45:44) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 13 basic blocks.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:232) automatically.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' (firmware/nnet_utils/nnet_pooling_stream.h:65:5)...3 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' (firmware/nnet_utils/nnet_pooling_stream.h:65:5)...3 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' (firmware/nnet_utils/nnet_dense_resource.h:44:17)...2551 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' (firmware/nnet_utils/nnet_dense_latency.h:17:13)...100 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:33:5)...3197 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:33:5)...202 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:05:59 ; elapsed = 00:06:03 . Memory (MB): peak = 9195.148 ; gain = 8780.051 ; free physical = 3813 ; free virtual = 107456\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_pooling_stream.h:241:66) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_pooling_stream.h:241:66) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:79:66) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:79:66) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>'.\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::softmax_stable<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' to 'softmax_stable<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:43:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::softmax<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config12>' to 'softmax<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>' (firmware/nnet_utils/nnet_activation_stream.h:362:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, relu_config3>' to 'relu<array<ap_fixed,8u>,array<ap_fixed<16,6,5,3,0>,8u>,relu_config3>' (firmware/nnet_utils/nnet_activation_stream.h:41:55)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, relu_config6>' to 'relu<array<ap_fixed,16u>,array<ap_fixed<16,6,5,3,0>,16u>,relu_config6>' (firmware/nnet_utils/nnet_activation_stream.h:41:55)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, relu_config10>' to 'relu<array<ap_fixed,10u>,array<ap_fixed<16,6,5,3,0>,10u>,relu_config10>' (firmware/nnet_utils/nnet_activation_stream.h:41:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' (firmware/nnet_utils/nnet_common.h:36:44)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config4>' to 'pooling2d_cl<array<ap_fixed,8u>,array<ap_fixed<16,6,5,3,0>,8u>,config4>' (firmware/nnet_utils/nnet_pooling_stream.h:65:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::pooling2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config7>' to 'pooling2d_cl<array,array<ap_fixed<16,6,5,3,0>,16u>,config7>' (firmware/nnet_utils/nnet_pooling_stream.h:65:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9>' to 'dense_wrapper<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config9>' (firmware/nnet_utils/nnet_dense_stream.h:13)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' to 'dense_wrapper<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>' (firmware/nnet_utils/nnet_dense_stream.h:13)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config9>' to 'dense<array<ap_fixed,16u>,array<ap_fixed<16,6,5,3,0>,10u>,config9>' (firmware/nnet_utils/nnet_dense_stream.h:36:39)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config11>' to 'dense<array<ap_fixed,10u>,array<ap_fixed<16,6,5,3,0>,10u>,config11>' (firmware/nnet_utils/nnet_dense_stream.h:36:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' to 'conv_2d_cl<array<ap_fixed,8u>,array<ap_fixed<16,6,5,3,0>,16u>,config5>' (firmware/nnet_utils/nnet_conv_stream.h:33:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 1u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config2>' to 'conv_2d_cl<array<ap_fixed,1u>,array<ap_fixed<16,6,5,3,0>,8u>,config2>' (firmware/nnet_utils/nnet_conv_stream.h:33:5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [XFORM 203-532] Ignored the rewind option for Function 'dense_wrapper<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config9>' (firmware/nnet_utils/nnet_dense_stream.h:13) as the option is not applicable to function pipelining.\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:08:03 ; elapsed = 00:08:07 . Memory (MB): peak = 9195.148 ; gain = 8780.051 ; free physical = 3753 ; free virtual = 107441\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array<ap_fixed,1u>,array<ap_fixed<16,6,5,3,0>,8u>,config2>' to 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,8u>,array<ap_fixed<16,6,5,3,0>,8u>,relu_config3>' to 'relu_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_relu_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'pooling2d_cl<array<ap_fixed,8u>,array<ap_fixed<16,6,5,3,0>,8u>,config4>' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array<ap_fixed,8u>,array<ap_fixed<16,6,5,3,0>,16u>,config5>' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,16u>,array<ap_fixed<16,6,5,3,0>,16u>,relu_config6>' to 'relu_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_16u_relu_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'pooling2d_cl<array,array<ap_fixed<16,6,5,3,0>,16u>,config7>' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_wrapper<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config9>' to 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array<ap_fixed,16u>,array<ap_fixed<16,6,5,3,0>,10u>,config9>' to 'dense_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_10u_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array<ap_fixed,10u>,array<ap_fixed<16,6,5,3,0>,10u>,relu_config10>' to 'relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_wrapper<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>' to 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config11_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array<ap_fixed,10u>,array<ap_fixed<16,6,5,3,0>,10u>,config11>' to 'dense_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_config11_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' to 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax_stable<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>' to 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>' to 'softmax_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 487.5 seconds; current allocated memory: 667.714 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.65 seconds; current allocated memory: 674.563 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_relu_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.62 seconds; current allocated memory: 675.198 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.09 seconds; current allocated memory: 675.575 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 676.338 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 677.284 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 10.01 seconds; current allocated memory: 741.548 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 75.66 seconds; current allocated memory: 847.957 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_16u_relu_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 27.96 seconds; current allocated memory: 855.406 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 856.065 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.28 seconds; current allocated memory: 857.403 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 859.138 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 5.68 seconds; current allocated memory: 902.266 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 36.09 seconds; current allocated memory: 949.119 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_10u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'DataPrepare'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 22.8 seconds; current allocated memory: 954.406 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.33 seconds; current allocated memory: 958.727 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'relu<array<ap_fixed,10u>,array<ap_fixed<16,6,5,3,0>,10u>,relu_config10>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.4 seconds; current allocated memory: 960.605 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 961.020 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'dense_wrapper<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.27 seconds; current allocated memory: 962.478 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 964.923 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 965.268 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 965.686 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 966.167 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.12 seconds; current allocated memory: 966.471 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'softmax_stable<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>'.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s' (Function: softmax_stable<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation_stream.h:232) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation ('exp_res_0_V_write_ln225', firmware/nnet_utils/nnet_activation_stream.h:225) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation_stream.h:225 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation_stream.h:219.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s' (Function: softmax_stable<array,array<ap_fixed<16,6,5,3,0>,10u>,softmax_config12>): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation_stream.h:232) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation ('exp_res_0_V_write_ln225', firmware/nnet_utils/nnet_activation_stream.h:225) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation_stream.h:225 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation_stream.h:219.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 3, Depth = 11.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.24 seconds; current allocated memory: 967.574 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.56 seconds; current allocated memory: 969.073 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 969.412 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 969.784 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.23 seconds; current allocated memory: 970.679 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 12.02 seconds; current allocated memory: 989.649 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'pX_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1273' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_6' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2274' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_7' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3275' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_24' is power-on initialization.\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffer_Array_V_0_0' to 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffbkb' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffer_Array_V_1270_0' to 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffcud' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffer_Array_V_2271_0' to 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffdEe' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffer_Array_V_3272_0' to 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s_line_buffeOg' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_ap_fixed_1u_array_ap_fixed_16_6_5_3_0_8u_config2_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 7.16 seconds; current allocated memory: 1005.178 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_relu_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_relu_config3_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.53 seconds; current allocated memory: 1.000 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'pX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_31' is power-on initialization.\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_0' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_bufYi' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_1' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_bug8j' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_2' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buhbi' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_3' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buibs' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_4' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_bujbC' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_5' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_bukbM' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_6' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_bulbW' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_buffer_Array_V_3_0_7' to 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s_line_bumb6' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mux_42_16_1_1': 8 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.35 seconds; current allocated memory: 1.003 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_48' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_49' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_50' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_51' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_52' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_53' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_54' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_55' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_88' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_89' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_90' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_91' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_92' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_93' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_94' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_95' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_128' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_129' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_130' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_131' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_132' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_133' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_134' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_135' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_168' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_169' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_170' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_171' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_172' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_173' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_174' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_175' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_56' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_57' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_58' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_59' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_60' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_61' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_62' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_63' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_96' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_97' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_98' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_99' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_100' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_101' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_102' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_103' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_136' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_137' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_138' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_139' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_140' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_141' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_142' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_143' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_176' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_177' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_178' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_179' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_180' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_181' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_182' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_183' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_31' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_64' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_65' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_66' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_67' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_68' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_69' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_70' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_71' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_104' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_105' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_106' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_107' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_108' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_109' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_110' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_111' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_144' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_145' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_146' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_147' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_148' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_149' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_150' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_151' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_184' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_185' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_186' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_187' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_188' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_189' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_190' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_191' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_32' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_33' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_34' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_35' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_36' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_37' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_38' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_39' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_72' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_73' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_74' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_75' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_76' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_77' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_78' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_79' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_112' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_113' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_114' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_115' is power-on initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_116' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_117' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_118' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_119' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_152' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_153' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_154' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_155' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_156' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_157' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_158' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_159' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_192' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_193' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_194' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_195' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_196' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_197' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_198' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_199' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_40' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_41' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_42' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_43' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_44' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_45' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_46' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_47' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_80' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_81' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_82' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_83' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_84' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_85' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_86' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_87' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_120' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_121' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_122' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_123' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_124' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_125' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_126' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_127' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_160' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_161' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_162' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_163' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_164' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_165' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_166' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_167' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_12' is power-on initialization.\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_0' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufncg' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_0' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufocq' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_0' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufpcA' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_0' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufqcK' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_1' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufrcU' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_1' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufsc4' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_1' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buftde' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_1' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufudo' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_2' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufvdy' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_2' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufwdI' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_2' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufxdS' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_2' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufyd2' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_3' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufzec' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_3' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufAem' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_3' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufBew' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_3' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufCeG' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_4' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufDeQ' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_4' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufEe0' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_4' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufFfa' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_4' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufGfk' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_5' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufHfu' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_5' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufIfE' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_5' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufJfO' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_5' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufKfY' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_6' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufLf8' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_6' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufMgi' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_6' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufNgs' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_6' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufOgC' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_0_7' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufPgM' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_1_7' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufQgW' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_2_7' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufRg6' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_buffer_Array_V_1_3_7' to 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s_line_bufShg' due to the length limit 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [RTGEN 206-104] Estimated max fanout for 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s' is 45168 from HDL expression: ((1'b0 == ap_block_pp0_stage0) & (ap_enable_reg_pp0_iter2 == 1'b1))\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.09 seconds; current allocated memory: 1.102 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_16u_relu_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_16u_relu_config6_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 47.43 seconds; current allocated memory: 1.388 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'pX_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_31' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_48' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_49' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_50' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_51' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_52' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_53' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_54' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_55' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_56' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_57' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_58' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_59' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_60' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_61' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_62' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_63' is power-on initialization.\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_0' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_Thq' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_1' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_UhA' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_2' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_VhK' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_3' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_WhU' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_4' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_Xh4' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_5' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_Yie' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_6' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_Zio' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_7' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_0iy' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_8' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_1iI' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_9' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_2iS' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_10' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_3i2' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_11' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_4jc' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_12' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_5jm' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_13' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_6jw' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_14' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_7jG' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_V_2_0_15' to 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s_line_buffer_Array_8jQ' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mux_42_16_1_1': 16 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.54 seconds; current allocated memory: 1.392 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config9_s' is 115571 from HDL expression: (1'b1 == ap_CS_fsm_state2)\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_10ns_26_1_1': 197 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_10s_26_1_1': 197 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_11ns_26_1_1': 18 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_11s_26_1_1': 16 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_6ns_22_1_1': 50 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_6s_22_1_1': 36 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_7ns_23_1_1': 124 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_7s_23_1_1': 133 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_8ns_24_1_1': 259 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_8s_24_1_1': 247 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_9ns_25_1_1': 339 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_16s_9s_25_1_1': 332 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config9_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 9.47 seconds; current allocated memory: 1.487 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_10u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_10u_config9_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 37.4 seconds; current allocated memory: 1.695 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 7.22 seconds; current allocated memory: 1.712 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_wrapper_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config11_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.85 seconds; current allocated memory: 1.715 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_config11_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.65 seconds; current allocated memory: 1.723 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mux_104_18_1_0': 4 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.01 seconds; current allocated memory: 1.724 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s_exp_table3' to 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s_exp_tab9j0' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s_invert_table4' to 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s_invert_bak' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.08 seconds; current allocated memory: 1.728 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.68 seconds; current allocated memory: 1.734 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/conv1_input_V_data_0_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_0_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_1_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_2_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_3_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_4_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_5_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_6_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_7_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_8_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer12_out_V_data_9_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4_U0' to 'start_for_pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4bbk' due to the length limit 80\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10_U0' to 'start_for_relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10bck' due to the length limit 80\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.34 seconds; current allocated memory: 1.739 GB.\n",
      "INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.\n",
      "INFO: [HLS 200-789] **** Estimated Fmax: 232.86 MHz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [RTMG 210-279] Implementing memory 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s_exp_tab9j0_rom' using block ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_stable_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_s_invert_bak_rom' using auto ROMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_0_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_1_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_2_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_3_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_4_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_5_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_6_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_7_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_0_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_1_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_2_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_3_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_4_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_5_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_6_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_7_V_U(fifo_w16_d576_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_0_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_1_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_2_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_3_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_4_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_5_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_6_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_7_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_0_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_1_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_2_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_3_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_4_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_5_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_6_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_7_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_8_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_9_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_10_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_11_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_12_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_13_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_14_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_15_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_0_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_1_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_2_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_3_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_4_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_5_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_6_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_7_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_8_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_9_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_10_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_11_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_12_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_13_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_14_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_15_V_U(fifo_w16_d64_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_0_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_1_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_2_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_3_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_4_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_5_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_6_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_7_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_8_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_9_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_10_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_11_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_12_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_13_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_14_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_15_V_U(fifo_w16_d16_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_relu_config3_U0_U(start_for_relu_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_relu_config3_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4bbk_U(start_for_pooling2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_8u_config4bbk)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_U0_U(start_for_conv_2d_cl_array_ap_fixed_8u_array_ap_fixed_16_6_5_3_0_16u_config5_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_16u_relu_config6_U0_U(start_for_relu_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_16u_relu_config6_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_U0_U(start_for_pooling2d_cl_array_array_ap_fixed_16_6_5_3_0_16u_config7_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_10u_config9_U0_U(start_for_dense_array_ap_fixed_16u_array_ap_fixed_16_6_5_3_0_10u_config9_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10bck_U(start_for_relu_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_relu_config10bck)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_config11_U0_U(start_for_dense_array_ap_fixed_10u_array_ap_fixed_16_6_5_3_0_10u_config11_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_softmax_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_U0_U(start_for_softmax_array_array_ap_fixed_16_6_5_3_0_10u_softmax_config12_U0)' using Shift Registers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:13:51 ; elapsed = 00:14:15 . Memory (MB): peak = 9195.148 ; gain = 8780.051 ; free physical = 603 ; free virtual = 105502\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h14m13s *****\n",
      "***** VIVADO SYNTHESIS *****\n",
      "\n",
      "****** Vivado v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source vivado_synth.tcl\n",
      "# set tcldir [file dirname [info script]]\n",
      "# source [file join $tcldir project.tcl]\n",
      "## variable project_name\n",
      "## set project_name \"myproject\"\n",
      "## variable backend\n",
      "## set backend \"vivado\"\n",
      "## variable part\n",
      "## set part \"xcku115-flvb2104-2-i\"\n",
      "## variable clock_period\n",
      "## set clock_period 5\n",
      "## variable clock_uncertainty\n",
      "## set clock_uncertainty 12.5%\n",
      "# add_files ${project_name}_prj/solution1/syn/vhdl\n",
      "# synth_design -top ${project_name} -part $part\n",
      "Command: synth_design -top myproject -part xcku115-flvb2104-2-i\n",
      "Starting synth_design\n",
      "Attempting to get a license for feature 'Synthesis' and/or device 'xcku115'\n",
      "WARNING: [Common 17-348] Failed to get the license for feature 'Synthesis' and/or device 'xcku115'. Explanation: The license feature Synthesis could not be found.\n",
      "Resolution: Check the status of your licenses in the Vivado License Manager. For debug help search Xilinx Support for \"Licensing FAQ\". \n",
      "0 Infos, 1 Warnings, 0 Critical Warnings and 1 Errors encountered.\n",
      "synth_design failed\n",
      "INFO: [Common 17-206] Exiting Vivado at Wed Jun 21 01:20:52 2023...\n",
      "ERROR: [Common 17-345] A valid license was not found for feature 'Synthesis' and/or device 'xcku115'. Please run the Vivado License Manager for assistance in determining\n",
      "which features and devices are licensed for your system.\n",
      "Resolution: Check the status of your licenses in the Vivado License Manager. For debug help search Xilinx Support for \"Licensing FAQ\". If you are using a license server, verify that the license server is up and running a version of the xilinx daemon that is compatible with the version of Xilinx software that you are using. Please note that Vivado 2017.3 and later requires upgrading your license server tools to the Flex 11.14.1 tools. Please confirm with your license admin that the correct version of the license server tools are installed.\n",
      "    while executing\n",
      "\"exec vivado -mode batch -source vivado_synth.tcl >@ stdout\"\n",
      "    invoked from within\n",
      "\"if {$opt(vsynth)} {\n",
      "    puts \"***** VIVADO SYNTHESIS *****\"\n",
      "    if {[file exist ${project_name}_prj/solution1/syn/vhdl]} {\n",
      "        set time_start [clo...\"\n",
      "    (file \"build_prj.tcl\" line 237)\n",
      "    invoked from within\n",
      "\"source build_prj.tcl\"\n",
      "    (\"uplevel\" body line 1)\n",
      "    invoked from within\n",
      "\"uplevel \\#0 [list source $arg] \"\n",
      "\n",
      "INFO: [HLS 200-112] Total elapsed time: 881.79 seconds; peak allocated memory: 1.739 GB.\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Wed Jun 21 01:21:03 2023...\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n",
       "  'EstimatedClockPeriod': '4.295',\n",
       "  'BestLatency': '807',\n",
       "  'WorstLatency': '807',\n",
       "  'IntervalMin': '789',\n",
       "  'IntervalMax': '789',\n",
       "  'BRAM_18K': '59',\n",
       "  'DSP': '4727',\n",
       "  'FF': '131881',\n",
       "  'LUT': '177805',\n",
       "  'URAM': '0',\n",
       "  'AvailableBRAM_18K': '4320',\n",
       "  'AvailableDSP': '5520',\n",
       "  'AvailableFF': '1326720',\n",
       "  'AvailableLUT': '663360',\n",
       "  'AvailableURAM': '0'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.build(csim=False, synth=True, vsynth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report( 'model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReports(indir):\n",
    "    data_ = {}\n",
    "\n",
    "    report_vsynth = Path('{}/vivado_synth.rpt'.format(indir))\n",
    "    report_csynth = Path('{}/myproject_prj/solution1/syn/report/myproject_csynth.rpt'.format(indir))\n",
    "\n",
    "    if report_vsynth.is_file() and report_csynth.is_file():\n",
    "        print('Found valid vsynth and synth in {}! Fetching numbers'.format(indir))\n",
    "\n",
    "        # Get the resources from the logic synthesis report\n",
    "        with report_vsynth.open() as report:\n",
    "            lines = np.array(report.readlines())\n",
    "            data_['lut'] = int(lines[np.array(['CLB LUTs*' in line for line in lines])][0].split('|')[2])\n",
    "            data_['ff'] = int(lines[np.array(['CLB Registers' in line for line in lines])][0].split('|')[2])\n",
    "            data_['bram'] = float(lines[np.array(['Block RAM Tile' in line for line in lines])][0].split('|')[2])\n",
    "            data_['dsp'] = int(lines[np.array(['DSPs' in line for line in lines])][0].split('|')[2])\n",
    "            data_['lut_rel'] = float(lines[np.array(['CLB LUTs*' in line for line in lines])][0].split('|')[5])\n",
    "            data_['ff_rel'] = float(lines[np.array(['CLB Registers' in line for line in lines])][0].split('|')[5])\n",
    "            data_['bram_rel'] = float(lines[np.array(['Block RAM Tile' in line for line in lines])][0].split('|')[5])\n",
    "            data_['dsp_rel'] = float(lines[np.array(['DSPs' in line for line in lines])][0].split('|')[5])\n",
    "\n",
    "        with report_csynth.open() as report:\n",
    "            lines = np.array(report.readlines())\n",
    "            lat_line = lines[np.argwhere(np.array(['Latency (cycles)' in line for line in lines])).flatten()[0] + 3]\n",
    "            data_['latency_clks'] = int(lat_line.split('|')[2])\n",
    "            data_['latency_mus'] = float(lat_line.split('|')[2]) * 5.0 / 1000.0\n",
    "            data_['latency_ii'] = int(lat_line.split('|')[6])\n",
    "\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pprint\n",
    "\n",
    "data_pruned_ref = getReports('pruned_cnn')\n",
    "data_quantized_pruned = getReports('quantized_pruned_cnn')\n",
    "\n",
    "print(\"\\n Resource usage and latency: Pruned\")\n",
    "pprint.pprint(data_pruned_ref)\n",
    "print(\"\\n Resource usage and latency: Pruned + quantized\")\n",
    "pprint.pprint(data_quantized_pruned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#////////////////////////////////////////////////////////////////End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_one_hot = loaded_model.predict([x_test_normalized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('predictions_one_hot:', predictions_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each prediction consists of 10 probabilities (one for each number from `0` to `9`). We need to pick the digit with the highest probability since this would be a digit that our model most confident with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions in form of one-hot vectors (arrays of probabilities).\n",
    "pd.DataFrame(predictions_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract predictions with highest probabilites and detect what digits have been actually recognized.\n",
    "predictions = np.argmax(predictions_one_hot, axis=1)\n",
    "pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model is predicting that the first example from the test set is `7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first image from a test set to see if model's prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test_normalized[0].reshape((IMAGE_WIDTH, IMAGE_HEIGHT)), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model made a correct prediction and it successfully recognized digit `7`. Let's print some more test examples and correspondent predictions to see how model performs and where it does mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_display = 196\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(x_test_normalized[plot_index].reshape((IMAGE_WIDTH, IMAGE_HEIGHT)), cmap=color_map)\n",
    "    plt.xlabel(predicted_label)\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a confusion matrix\n",
    "\n",
    "[Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows what numbers are recognized well by the model and what numbers the model usually confuses to recognize correctly. You may see that the model performs really well but sometimes (28 times out of 10000) it may confuse number `5` with `3` or number `2` with `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.5,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging the model with TensorBoard\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard) is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir .logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model to web-format\n",
    "\n",
    "To use this model on the web we need to convert it into the format that will be understandable by [tensorflowjs](https://www.tensorflow.org/js). To do so we may use [tfjs-converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter) as following:\n",
    "\n",
    "```\n",
    "tensorflowjs_converter --input_format keras \\\n",
    "  ./experiments/digits_recognition_cnn/digits_recognition_cnn.h5 \\\n",
    "  ./demos/public/models/digits_recognition_cnn\n",
    "```\n",
    "\n",
    "You find this experiment in the [Demo app](https://trekhleb.github.io/machine-learning-experiments) and play around with it right in you browser to see how the model performs in real life."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
